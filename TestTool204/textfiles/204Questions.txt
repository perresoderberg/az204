#
#
#
#
#
#
#
#
#
#
#
***********************
N:1
T VM
You have two Hyper-V hosts named Host1 and Host2. Host1 has an Azure virtual machine named VM1 that was deployed by using a custom Azure Resource Manager template.
You need to move VM1 to Host2.
What should you do?
1 From the Update management blade, click Enable.
2 From the Overview blade, move VM1 to a different subscription.
3 From the Redeploy blade, click Redeploy.
4 From the Profile blade, modify the usage location.
ATYPE:SingleSelect
ANSWER:3
C. When you redeploy a VM, it moves the VM to a new node within the Azure infrastructure and then powers it back on,
retaining all your configuration options and associated resources.
https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/redeploy-to-new-node-windows
***********************
N:2
T security
You have downloaded an Azure Resource Manager template to deploy numerous virtual machines.
The template is based on a current virtual machine, but must be adapted to reference an administrative password.
You need to make sure that the password is not stored in plain text.
Which of the following components should you create to achieve your goal? 
* An Azure key vault
* An Azure Storage account
* Azure Active Directory Identity Protection
* An access Policy
* An Azure policy
* A backup policy
ATYPE:MULTISELECT
ANSWER:1,4
Key Vault + Access Policy.
Using Key Vault we create a secret containing our Password.
Using an Access Policy we allow access to the previously created secret.

Key Vault =>     https://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-portal
Access Policy => https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/key-vault-parameter?tabs=azure-cli

https://learn.microsoft.com/en-us/azure/key-vault/general/assign-access-policy
***********************
N:3
T webapp
Your company has a web app named WebApp1.
You use the WebJobs SDK to design a triggered App Service background task that automatically invokes a function in the code every time new data is received in a queue.
You are preparing to configure the service processes a queue data item.
Which of the following is the service you should use?
* Logic Apps
* WebJobs
* Flow
* Functions
ATYPE:SINGLESELECT
ANSWER:2
WebJobs
You use the WebJobs SDK to design a triggered App Service background task
Azure WebJobs is not a service, you'll host the WebJobs SDK functions in Azure WebApps.
With Webjobs you program code in your webapp and you'll be able to execute 'kind of Azure functions' inside your Web App.
It is automatically deployed with the deployment of the web app. WebJobs is the predecessor of Azure functions.

https://github.com/Azure/azure-webjobs-sdk

***********************
N:4
T webapp
You have developed a Web App for your company. The Web App provides services and must run in multiple regions.
You want to be notified whenever the Web App uses more than 85 percent of the available CPU cores over a 5 minute period. Your solution must minimize costs.
Which command should you use?
*az monitor metrics alert create -n myAlert -g group --scope targetResourceID --condition dropdown1 >85
dropdown2 5m
1A CPU Usage
1B Percentage CPU
1C avg Percentage CPU
2A --window size
2B --evaluation-frequency
2C --auto-mitigate
Atype:dropdown
Answer:1C,2A
avg Percentage CPU
--window size
For anyone wondering why it's --window-size and not --evaluation-frequency: you want the average across 5 minutes. With --evaluation-frequency you don't go for averages, you simply check what the given value is at specific intervals.

https://docs.microsoft.com/en-us/cli/azure/monitor/metrics/alert?view=azure-cli-latest
https://learn.microsoft.com/en-us/cli/azure/monitor/metrics/alert?view=azure-cli-latest#az-monitor-metrics-alert-create
***********************
N:5
T webapp
You are developing an Azure Web App. You configure TLS mutual authentication for the web app.
You need to validate the client certificate in the web app. 
1 1_[Client certificate location]
2 1_HTTP request header
3 1_Client cookie
4 1_HTTP message body
5 1_URL query string
6 2_[Encoding type]
7 2_HTML
8 2_URL
9 2_Unicode
10 2_Base64
ATYPE:MULTISELECT
ANSWER:2,9
HTTP request header
If you are using ASP.NET and configure your app to use client certificate authentication, the certificate will be available through the HttpRequest.ClientCertificate property.
Base64
For other application stacks, the client cert will be available in your app through a base64 encoded value in the "X-ARR-ClientCert" request header. 
Your application can create a certificate from this value and then use it for authentication and authorization purposes in your application.

https://docs.microsoft.com/en-us/azure/app-service/app-service-web-configure-tls-mutual-auth

With client certificates enabled, App Service injects an X-ARR-ClientCert request header with the client certificate.
***********************
N:6
T Docker
Fourth Coffee has an ASP.NET Core web app that runs in Docker. The app is mapped to the www.fourthcoffee.com domain.
Fourth Coffee is migrating this application to Azure.
You need to provision an App Service Web App to host this docker image and map the custom domain to the App Service web app.
A resource group named FourthCoffeePublicWebResourceGroup has been created in the WestUS region that contains an App Service Plan named
AppServiceLinuxDockerPlan.
Which order should the CLI commands be used to develop the solution?
group-1-start
az webapp config container set
   --docker-custom-image-name
  $dockerHubContainerPath
   --name $appName
   -g fourthCoffeePublicWebResourceGroup
group-1-end
group-2-start
az webapp config hostname add
 --webapp-name $appName
 -g fourthCoffeePublicWebResourceGroup --hostname $fqdn
group-2-end
group-3-start
az webapp create
 --name $appName
 --plan AppSerivceLinuxDockerPlan
 -g fourthCoffeePublicWebResourceGroup 
group-3-end
group-4-start
#/bin/bash
appName="fourthCoffeePublicWeb$random"
location="WestUs"
dockerHubContainerPath="FourthCoffee/publicweb:v1"
fqdn="http://www.fourthcoffee.com">www.fourthcoffee.com
group-4-end
ATYPE:ORDER
ANSWER:4,3,1,2
d, c, a, b
https://docs.microsoft.com/en-us/azure/app-service/app-service-web-tutorial-custom-domain
https://docs.microsoft.com/en-us/azure/app-service/containers/tutorial-custom-docker-image
https://docs.microsoft.com/en-us/azure/app-service/tutorial-custom-container?pivots=container-linux
https://docs.microsoft.com/en-us/azure/app-service/scripts/cli-configure-custom-domain
***********************
N:7
T Cosmos
You are creating an Azure Cosmos DB account that makes use of the SQL API. Data will be added to the account every day by a web application.
You need to ensure that an email notification is sent when information is received from IoT devices, and that compute cost is reduced.
You decide to deploy a function app.
Which of the following should you configure the function app to use?
1 Azure Cosmos DB connetor
2 SendGrid action
3 Consumption plan
4 Azure Event Hubs binding
5 SendGrid binding
Atype:multiselect
Answer:5,3
Consumption plan will reduce the cost.
SendGrid is used to send emails from azure functions. (to send email use SendGrid bindings in Azure Function.)

https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-sendgrid?tabs=in-process%2Cfunctionsv2&pivots=programming-language-csharp
https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-sendgrid?tabs=in-process%2Cfunctionsv2&pivots=programming-language-csharp#example
***********************

N:8
T VM
Your company has an Azure subscription.
You need to deploy a number of Azure virtual machines to the subscription by using Azure Resource Manager (ARM) templates. The virtual machines will be included in a single availability set.
You need to ensure that the ARM template allows for as many virtual machines as possible to remain accessible in the event of fabric failure or maintenance.
Which of the following is the value that you should configure for the platformFaultDomainCount property?
* 10
* 30
* Min Value
* Max Value
ATYPE:SINGLESELECT
ANSWER:4
Max Value. The number of fault domains for managed availability sets varies by region - either two or three per region.

https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-manage-fault-domains

https://docs.microsoft.com/en-us/azure/virtual-machines/availability-set-overview
Each virtual machine in your availability set is assigned an update domain and a fault domain by the underlying Azure platform.
Each availability set can be configured with up to three fault domains and twenty update domains.
***********************
N:9
T VM
Your company has an Azure subscription.
You need to deploy a number of Azure virtual machines to the subscription by using Azure Resource Manager (ARM) templates. The virtual machines will be included in a single availability set.
You need to ensure that the ARM template allows for as many virtual machines as possible to remain accessible in the event of fabric failure or maintenance.
Which of the following is the value that you should configure for the platformUpdateDomainCount property?
* 10
* 20
* 30
* 40
ATYPE:SINGLESELECT
ANSWER:1
A
20. Each availability set can be configured with up to three fault domains and twenty update domains.

https://docs.microsoft.com/en-us/azure/virtual-machines/availability-set-overview
***********************
N:10
T cosmos
This question requires that you evaluate the underlined text to determine if it is correct.
You company has an on-premises deployment of MongoDB, and an Azure Cosmos DB account that makes use of the MongoDB API.
You need to devise a strategy to migrate MongoDB to the Azure Cosmos DB account.
You include the Data Management Gateway tool in your migration strategy.
* No change required
* mongorestore
* Azure Storage Explorer
* AzCopy
ATYPE:SINGLESELECT
ANSWER:2
mongorestore. DMG can not have Cosmos DB - Mongo as a sync (target). Refer to the link you provided.
The Data management gateway is a client agent that you must install in your on-premises environment to copy data between cloud and on-premises data stores.
Data Management Gateway is not supported for Azure Cosmos DB

https://docs.microsoft.com/en-us/azure/cosmos-db/mongodb-pre-migration
https://docs.microsoft.com/en-us/azure/dms/tutorial-mongodb-cosmos-db-online
https://docs.microsoft.com/en-us/azure/data-factory/v1/data-factory-data-management-gateway
***********************
N:12
T webapp
You are developing an e-Commerce Web App.
You want to use Azure Key Vault to ensure that sign-ins to the e-Commerce Web App are secured by using Azure App Service authentication and Azure Active
Directory (AAD).
What should you do on the e-Commerce Web App?
1 Run the az keyvault secret command.
2 Enable Azure AD Connect.
3 Enable Managed Service Identity (MSI).
4 Create an Azure AD service principal.
ATYPE:SINGLESELECT
ANSWER:3
Enable Managed Service Identity (MSI).
A managed identity from Azure Active Directory allows your app to easily access other AAD-protected resources such as Azure Key Vault.
Managed identities for Azure resources allow for giving Azure services an automatically managed identitfy in Azure Active Directory (Azure AD).
Managed Service Identity (MSI) is a feature in Azure that allows you to securely authenticate an Azure service to other Azure services without having to manage credentials. By enabling MSI on the Azure App Service hosting the e-Commerce Web App, you can create a trust relationship between the App Service and Azure Key Vault. This allows the e-Commerce Web App to authenticate with Azure Active Directory (AAD) and securely retrieve secrets from the Key Vault.

https://docs.microsoft.com/en-us/azure/key-vault/general/tutorial-net-create-vault-azure-web-app
https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview
***********************
N:13
T webapp
Your Azure Active Directory Azure (Azure AD) tenant has an Azure subscription linked to it.
Your developer has created a mobile application that obtains Azure AD access tokens using the OAuth 2 implicit grant type.
The mobile application must be registered in Azure AD.
You require a redirect URI from the developer for registration purposes.
1 No change required.
2 a secret
3 a login hint
4 a client ID
ATYPE:SINGLESELECT
ANSWER:1
No change required.
You don't need client id to register an application in Azure AD. You just need redirect URI. Upon registration, the portal will give client id and tenant id. Both of them must be included in the app configuration json file, so given answer is correct as far as app registration is concerned.
When a user tries to authenticate and authorize an application to access their Azure AD resources, Azure AD sends an authentication token to the redirect URI that was specified during application registration. The application then extracts the token from the redirect URL and uses it to access Azure AD resources on behalf of the user.

https://docs.microsoft.com/en-us/learn/modules/secure-app-with-oidc-and-azure-ad/4-exercise-create-aad-register-app
https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app#add-a-redirect-uri
***********************
N:14
T security
You are creating an Azure key vault using PowerShell. Objects deleted from the key vault must be kept for a set period of 90 days.
Which two of the following parameters must be used in conjunction to meet the requirement? (Choose two.)
1 EnabledForDeployment
2 EnablePurgeProtection
3 EnabledForTemplateDeployment
4 EnableSoftDelete
ATYPE:MULTISELECT
ANSWER:2,4
B and D.
The EnablePurgeProtection parameter prevents the deletion of key vault objects by enabling purge protection. This parameter ensures that objects deleted from the key vault cannot be permanently deleted until the purge protection period has expired.
The EnableSoftDelete parameter allows the key vault to retain deleted objects for a specified duration. This parameter enables soft delete, which allows the key vault to retain deleted objects for a specified period of time (90 days in this case) before they are permanently deleted.

https://docs.microsoft.com/en-us/powershell/module/azurerm.keyvault/new-azurermkeyvault
https://docs.microsoft.com/en-us/azure/key-vault/key-vault-ovw-soft-delete
***********************
N:15
T
You have an Azure Active Directory (Azure AD) tenant.
You want to implement multi-factor authentication by making use of a conditional access policy. The conditional access policy must be applied to all users when they access the Azure portal.
Which three settings should you configure? To answer, select the appropriate settings in the answer area.
NOTE: Each correct selection is worth one point.
SEE PICTURE 13
Name: MFA
1 [Assignments]
2 Users and groups
3 Cloud apps
4 Conditions
5 [Access controls]
6 Grant
7 session
ATYPE:MULTISELECT
ANSWER:2,3,6
Users and groups
Cloud apps
Grant
Microsoft wants a developer to remember their unfortunate choice to put the requirement (condition) for MFA not in the "conditions" tab, but in the "grants"?

https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/howto-conditional-access-policy-azure-management
***********************
N:14
T
You manage an Azure SQL database that allows for Azure AD authentication.
You need to make sure that database developers can connect to the SQL database via Microsoft SQL Server Management Studio (SSMS).
You also need to make sure the developers use their on-premises Active Directory account for authentication. Your strategy should allow for authentication prompts to be kept to a minimum.
Which of the following should you implement?
1 Azure AD token.
2 Azure Multi-Factor authentication.
3 Active Directory integrated authentication.
4 OATH software tokens.
ATYPE:SINGLESELECT
ANSWER:3
SEE PICTURE 14
Active Directory integrated authentication
Active Directory integrated authentication is the correct option as it allows users to connect to the database using their Windows credentials, which are authenticated through their on-premises Active Directory. This option avoids the need for users to enter their credentials each time they connect to the database, reducing authentication prompts to a minimum.
Azure AD can be the initial Azure AD managed domain. Azure AD can also be an on-premises Active Directory Domain Services that is federated with the Azure
AD.
Using an Azure AD identity to connect using SSMS or SSDT
***********************
N:15
T
You are developing an application to transfer data between on-premises file servers and Azure Blob storage.
The application stores keys, secrets, and certificates in Azure Key Vault and makes use of the Azure Key Vault APIs.
You want to configure the application to allow recovery of an accidental deletion of the key vault or key vault objects for 90 days after deletion.
What should you do?
1 Run the Add-AzKeyVaultKey cmdlet.
2 Run the az keyvault update --enable-soft-delete true --enable-purge-protection true CLI.
3 Implement virtual network service endpoints for Azure Key Vault.
4 Run the az keyvault update --enable-soft-delete false CLI.
ATYPE:SINGLESELECT
ANSWER:2
B.
When soft-delete is enabled, resources marked as deleted resources are retained for a specified period (90 days by default). The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion.
Purge protection is an optional Key Vault behavior and is not enabled by default. Purge protection can only be enabled once soft-delete is enabled.
When purge protection is on, a vault or an object in the deleted state cannot be purged until the retention period has passed. Soft-deleted vaults and objects can still be recovered, ensuring that the retention policy will be followed.
The default retention period is 90 days, but it is possible to set the retention policy interval to a value from 7 to 90 days through the Azure portal. Once the retention policy interval is set and saved it cannot be changed for that vault.
***********************
N:16
T
You are configuring a web app that delivers streaming video to users. The application makes use of continuous integration and deployment.
You need to ensure that the application is highly available and that the users' streaming experience is constant.
You also want to configure the application to store data in a geographic location that is nearest to the user.
Which solutions does meet the goals?
1 You include the use of Azure Redis Cache in your design.
2 You include the use of an Azure Content Delivery Network (CDN) in your design.
3 You include the use of a Storage Area Network (SAN) in your design.
ATYPE:MULTISELECT
ANSWER:2
Solution 2
Streaming video files to the client on demand. Video benefits from the low latency and reliable connectivity available from the globally located datacenters that offer CDN connections. Microsoft Azure Media Services (AMS) integrates with Azure CDN to deliver content directly to the CDN for further distribution.
Azure Cache for Redis is Distributed, in-memory, scalable solution providing super-fast data access. I think it is more suitable for database query results or session data caching and not content like video/images which you just fetch from azure storage and display.

Using Azure Redis Cache alone would not provide the level of high availability and constant streaming experience that you are looking for. Redis Cache is a in-memory data store that can improve the performance of web applications by caching data that is frequently accessed. However, it does not provide geographic replication or automatic failover to ensure high availability.

Azure Content Delivery Network (CDN) can be used to deliver streaming video content to users with low latency by caching the video at edge locations that are nearest to the user. Azure Media Services can be used to encode, package, and stream video content. Azure Media Services also provides built-in redundancy and failover options to ensure high availability.
Another option is to store the video files in Azure Blob storage, which provides a high-availability and low-latency storage solution that can be replicated to multiple regions.

A Storage Area Network (SAN) is a dedicated network that provides access to consolidated, block-level data storage. It is used to increase the availability of data and improve the performance of applications that require access to shared data. However, SANs do not provide features to ensure high availability of web applications, nor do they provide a mechanism to store data in the geographic location nearest to the user.

https://docs.microsoft.com/en-us/azure/architecture/best-practices/cdn
https://docs.microsoft.com/en-us/azure/architecture/best-practices/cdn#how-and-why-a-cdn-is-used

What is the difference between Azure CDN and Azure Media Services?
https://docs.microsoft.com/en-us/azure/media-services/latest/media-services-overview
***********************
N:17
T
You develop a Web App on a tier D1 app service plan.
You notice that page load times increase during periods of peak traffic.
You want to implement automatic scaling when CPU load is above 80 percent. Your solution must minimize costs.
What should you do first?
1 Enable autoscaling on the Web App.
2 Switch to the Premium App Service tier plan.
3 Switch to the Standard App Service tier plan.
4 Switch to the Azure App Services consumption plan.
ATYPE:SINGLESELECT
ANSWER:3
B.
C
D1 is Shared Tier and does not offer autoscaling.
Configure the web app to the Standard App Service Tier. The Standard tier supports auto-scaling, and we should minimize the cost. We can then enable autoscaling on the web app, add a scale rule and add a Scale condition.

https://learn.microsoft.com/en-us/azure/azure-functions/consumption-plan
***********************
N:18
T
Your company's Azure subscription includes an Azure Log Analytics workspace.
Your company has a hundred on-premises servers that run either Windows Server 2012 R2 or Windows Server 2016, and is linked to the Azure Log Analytics workspace. The Azure Log Analytics workspace is set up to gather performance counters associated with security from these linked servers.
You must configure alerts based on the information gathered by the Azure Log Analytics workspace.
You have to make sure that alert rules allow for dimensions, and that alert creation time should be kept to a minimum. Furthermore, a single alert notification must be created when the alert is created and when the alert is resolved.
You need to make use of the necessary signal type when creating the alert rules.
Which of the following is the option you should use?
1 The Activity log signal type.
2 The Application Log signal type.
3 The Metric signal type.
4 The Audit Log signal type.
ATYPE:SINGLESELECT
ANSWER:3
C
Metric alerts in Azure Monitor provide a way to get notified when one of your metrics cross a threshold. Metric alerts work on a range of multi-dimensional platform metrics, custom metrics, Application Insights standard and custom metrics.
Note: Signals are emitted by the target resource and can be of several types. Metric, Activity log, Application Insights, and Log.
Metric Alerts are stateful - only notifying once when alert is fired and once when alert is resolved; as opposed to Log alerts, which are stateless and keep firing at every interval if the alert condition is met.
Metric Alerts for Log provide multiple dimensions, allowing filtering to specific values like Computers, OS Type, etc. simpler; without the need for defining a complex query in Log Analytics.

https://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-metric-logs#configuring-metric-alert-for-logs
Signal types:
* Activity Log ... includes service health records along with records on any configuration changes made to the resources (and is available to all Azure resources)
* Audit Log ... contains the history of sign-in activity and audit trail of changes made within a particular tenant
* Metric ... numerical values that are collected at regular intervals and describe some aspect of a system at a particular time
* Application Log ... ?

The Metrics feature can only store numeric data in a particular structure, whereas the Logs feature can store a variety of datatypes (each with its own structure).

https://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview
https://docs.microsoft.com/en-us/azure/azure-monitor/essentials/data-platform-metrics
https://docs.microsoft.com/en-us/azure/azure-monitor/logs/data-platform-logs
***********************
N:19
T
You are developing a solution for a public facing API.
The API back end is hosted in an Azure App Service instance. You have implemented a RESTful service for the API back end.
You must configure back-end authentication for the API Management service instance.
Which solution(s) are correct?
1 You configure Basic gateway credentials for the Azure resource.
2 You configure Client cert gateway credentials for the HTTP(s) endpoint.
3 You configure Basic gateway credentials for the HTTP(s) endpoint.
4 You configure Client cert gateway credentials for the Azure resource.
ATYPE:MULTISELECT
ANSWER:2,4
Solution 1 is NOT correct, but read this.
There is "Authenticate with Basic policy" for api-management
https://learn.microsoft.com/en-us/azure/api-management/api-management-authentication-policies#Basic
https://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-api-management-rest-api-backend-entity

Authentication policies:
- Authenticate with Basic -> Authenticate with a backend service using Basic authentication.
- Authenticate with client certificate -> Authenticate with a backend service using client certificates.
- Authenticate with managed identity -> Authenticate with the managed identity for the API Management service.

Basic client credentials can be defined for HTTP endpoint, not an App Service.
Configuring Basic gateway credentials for the Azure resource would provide authentication for accessing the Azure resource itself, but it does not provide authentication for the API Management service instance.

To configure back-end authentication for the API Management service instance, you should use one of the following authentication options:
Client Certificate authentication
Token-based authentication
OAuth 2.0 authentication
These options provide secure authentication and access control for the API Management service instance and its associated APIs.

Solution 2 is correct (This is probably correct)
Does app Service Support certificate auth? YES, built in the Azure portal directly as a setting for app Service.
Secure backend services using client certificate authentication in Azure API Management:

https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-mutual-certificates

https://docs.microsoft.com/en-us/azure/api-management/api-management-howto-mutual-certificates#configure-an-api-to-use-client-certificate-for-gateway-authentication

Solution 2 is NOT correct
Client certificate gateway credentials are used for client-side authentication, which is not the same as back-end authentication. Back-end authentication is used to authenticate the API Management service instance with the back-end service hosted in the Azure App Service instance.
For back-end authentication, you can use Azure Active Directory (AAD) authentication, Azure AD B2C, or OAuth 2.0 authentication to authenticate the API Management service instance with the back-end service.

Solution 3 is NOT correct
Configuring Basic gateway credentials for the HTTP(s) endpoint would not provide a secure solution for back-end authentication for the public facing API. Basic authentication sends the user's credentials in plain text over the network, making it vulnerable to eavesdropping and man-in-the-middle attacks. This is not suitable for public facing APIs.

Target: "Azure Logic App" or "HTTP(s) endpoint"
Gateway credentials: "None" or "Basic" or "Client cert"

https://docs.microsoft.com/en-us/azure/api-management/api-management-howto-mutual-certificates#configure-an-api-to-use-client-certificate-for-gateway-authentication

Solution 4 is correct
Client certificate authentication is a more secure mechanism than Basic authentication, as it uses a secure communication channel and cryptographic keys to authenticate clients. Additionally, it provides better scalability, as it does not require the gateway to maintain a large number of user credentials.
***********************
N:20
T
You are developing an application that applies a set of governance policies for internal and external services, as well as for applications.
You develop a stateful ASP.NET Core 2.1 web application named PolicyApp and deploy it to an Azure App Service Web App. The PolicyApp reacts to events from
Azure Event Grid and performs policy actions based on those events.
You have the following requirements:
(*) Authentication events must be used to monitor users when they sign in and sign out.
(*) All authentication events must be processed by PolicyApp.
(*) Sign outs must be processed as fast as possible.
What should you do?
1 Create a new Azure Event Grid subscription for all authentication events. Use the subscription to process sign-out events.
2 Create a separate Azure Event Grid handler for sign-in and sign-out events.
3 Create separate Azure Event Grid topics and subscriptions for sign-in and sign-out events.
4 Add a subject prefix to sign-out events. Create an Azure Event Grid subscription. Configure the subscription to use the subjectBeginsWith filter.
ATYPE:MULTISELECT
ANSWER:3,4
C or D
C:
Only C is mentioned both topic and subscription, which are two critical parts for event grid
D doesn't contain any action of Sign-in. I am more inclined to C
D:
Since the requirement states that all authentication events must be processed by PolicyApp, it is more appropriate to use a single Azure Event Grid subscription to process all authentication events. To differentiate between sign-in and sign-out events, a subject prefix can be added to sign-out events. This allows the use of the subjectBeginsWith filter to quickly identify and process sign-out events.
Only D provide for Us All statements:
D. Add a subject prefix to sign-out events. -> We can filter sign-out events later,
Create an Azure Event Grid subscription. -> We have subcription and processed by PolicyApp
Configure the subscription to use the subjectBeginsWith filter. -> filter to process sign out events
Event Grid event schema have subject attribute through which subject prefix can be added to sign-out events when publishing to custom topic. Create a Event subscription and configure it to use subjectBeginsWith attribute of subject filtering for to process signout as fast as possible.

https://learn.microsoft.com/en-us/azure/event-grid/event-schema
https://learn.microsoft.com/en-us/azure/event-grid/custom-topics
https://learn.microsoft.com/en-us/azure/event-grid/how-to-filter-events

https://docs.microsoft.com/en-us/azure/event-grid/event-filtering
https://docs.microsoft.com/en-us/azure/event-grid/concepts
***********************
N:21
T
You are developing a C++ application that compiles to a native application named process.exe. The application accepts images as input and returns images in one of the following image formats: GIF, PNG, or JPEG.
You must deploy the application as an Azure Function.
You need to configure the function and host json files.
How should you complete the json files? To answer, select the appropriate options in the answer area.

function.json
{
	dropdown1
	"direction": out"
	"name": "result"
}
host.json
	dropdown2
	"defaultExecutablePath": "process.exe"
	},
	dropdown3
}
1A "type": "http"
1B "platform": "gcm"
1C "datatype": "stream"
1D "path": "process.exe"
2A "customHandler": { "description": {
2B "languageWorker": { "path": {
2C "extentions": { "worker": {
2D "extentionBundle": {
3A "enableForwardingHttpRequest": true
3B "enableForwardingHttpRequest": false
Atype:dropdown
Answer:1A,2A,3A
"type": "http"
"customHandler": { "description": {
"enableForwardingHttpRequest": true
For HTTP-triggered functions with no additional bindings or outputs, you may want your handler to work directly with the HTTP request and response instead of the custom handler request and response payloads. This behavior can be configured in host.json using the enableForwardingHttpRequest setting.
At the root of the app, the host.json file is configured to run handler.exe and enableForwardingHttpRequest is set to true.

In the function.json file, enableForwardingHttpRequest should be set to true for the Azure Function to accept HTTP requests and forward them to the C++ application for processing.
This is because the C++ application is compiled as a native application and cannot directly receive HTTP requests. By setting enableForwardingHttpRequest to true, the Azure Function acts as a proxy and forwards incoming HTTP requests to the C++ application.

https://learn.microsoft.com/en-us/azure/azure-functions/functions-custom-handlers
***********************
N:22
T
You are developing an Azure Static Web app that contains training materials for a tool company. Each tool’s training material is contained in a static web page that is linked from the tool’s publicly available description page.
A user must be authenticated using Azure AD prior to viewing training.
You need to ensure that the user can view training material pages after authentication.
How should you complete the configuration file? To answer, select the appropriate options in the answer area.
{
dropdown1
dropdown2
dropdown3
dropdown4
1A "routes": {
1B "headers": {
1C "responseOverrides": {
1D "navigationFallback": {
2A "400": {
2B "401": {
2C "403": {
2D "404": {
3A "statusCode": "301"
3B "statusCode": "304"
3C "statusCode": "401"
3D "statusCode": "302"
4A "redirect": "/.auth/login/aad?post_login_redirect_uri=.302orig"
4B "redirect": "/.auth/login/aad?post_login_redirect_uri=.302route"
4C "redirect": "/.auth/login/aad?post_login_redirect_uri=.302return"
4D "redirect": "/.auth/login/aad?post_login_redirect_uri=.referrer"
Atype:dropdown
Answer:1C,2B,3D,4D
"responseOverrides": {
"401": {
"statusCode": "302"
"redirect": "/.auth/login/aad?post_login_redirect_uri=.referrer"

https://learn.microsoft.com/en-us/azure/static-web-apps/authentication-authorization
https://learn.microsoft.com/en-us/azure/static-web-apps/configuration
https://learn.microsoft.com/en-us/azure/static-web-apps/configuration#restrict-access-to-entire-application
https://learn.microsoft.com/en-us/azure/static-web-apps/configuration#response-overrides
***********************
N:23
T
You are authoring a set of nested Azure Resource Manager templates to deploy Azure resources. You author an Azure Resource Manager template named mainTemplate.json that contains the following linked templates: linkedTemplate1.json, linkedTemplate2.json.
You add parameters to a parameters template file named mainTemplate.parameters,json. You save all templates on a local device in the C:\templates\ folder.

You have the following requirements:
(*) Store the templates in Azure for later deployment.
(*) Enable versioning of the templates.
(*) Manage access to the templates by using Azure RBAC.
(*) Ensure that users have read-only access to the templates.
(*) Allow users to deploy the templates.

You need to store the templates in Azure.
How should you complete the command? To answer, select the appropriate options in the answer area.

dropdown1
--name templateScore --version "1.0" -g templateRG --l "eastus"
dropdown2
"c:\templates\
--tags Dept=HumanResources Environment=Production
1A az ts create
1B az storage account create
1C az storage account update
1D az blueprint artifact template create
2A --template-file mainTemplate.json
2B --template-file linkedTemplate1.json
2C --template-file linkedTemplate2.json
2D --template-file mainTemplate.parameters.json
Atype:dropdown
Answer:1A,2A
az ts create
--template-file mainTemplate.json

https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/template-specs-create-linked?tabs=azure-cli
https://learn.microsoft.com/en-us/cli/azure/ts?view=azure-cli-latest

'az ts' allows you to manage template specs at subscription or resource group scope.
***********************
N:24
T
You are developing a service where customers can report news events from a browser using Azure Web PubSub.
The service is implemented as an Azure Function App that uses the JSON WebSocket subprotocol to receive news events.
You need to implement the bindings for the Azure Function App.
How should you configure the binding?

"bindings": [
{
"type": "dropdown1",
"direction": "in",
"name": "data",
"eventName": "message",
"eventType": "dropdown2",
}}}
1A user
1B system
1C message
1D connected
1E webPubSubTrigger
1F webPubSubConnection
2A user
2B system
2C message
2D connected
2E webPubSubTrigger
2F webPubSubConnection
Atype:dropdown
Answer:1E,2A
webPubSubTrigger
user

https://learn.microsoft.com/en-us/azure/azure-web-pubsub/reference-functions-bindings?tabs=javascript
***********************
N:25
T
You are building a software-as-a-service (SaaS) application that analyzes DNA data that will run on Azure virtual machines (VMs) in an availability zone. The data is stored on managed disks attached to the VM. The performance of the analysis is determined by the speed of the disk attached to the VM.
You have the following requirements:
(*) The application must be able to quickly revert to the previous day’s data if a systemic error is detected.
(*) The application must minimize downtime in the case of an Azure datacenter outage.

You need to provision the managed disk for the VM to maximize performance while meeting the requirements.
Which type of Azure Managed Disk should you use?

Disc type dropdown1
Redundancy dropdown2

1A Premium SSD
1B Standard SSD
1C Standard HDD
2A Geo-redundant storage
2B Zone-redundant storage
2C Locally-redundant storage
Atype:dropdown
Answer:1E,2A
Premium SSD and ZRS
They are asking for high performance workloads which is supported by Premium tier

https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types

Also they are asking for zone redundancy (if datacenter goes down, NOT region outage).
Also managed disk doesn't support GRS

Azure managed disks offer two storage redundancy options, zone-redundant storage (ZRS), and locally-redundant storage. No geo

https://learn.microsoft.com/en-us/azure/virtual-machines/disks-redundancy
***********************
N:26
T
You are developing an application that includes two Docker containers.

The application must meet the following requirements:

(*) The containers must not run as root.
(*) The containers must be deployed to Azure Container Instances by using a YAML file.
(*) The containers must share a lifecycle, resources, local network, and storage volume.
(*) The storage volume must persist through container crashes.
(*) The storage volume must be deployed on stop or restart of the containers.

You need to configure Azure Container Instances for the application.

Which configuration values should you use?
Shared lifecycle [Container group]/[Container image]/[Service endpoint]/[Resource group]
Storage volume [Azure file share]/[Secret]/[Empty directory]/[Cloned Git repo]

1A Container group
1B Container image
1C Service endpoint
1D Resource group
2A Azure file share
2B Secret
2C Empty directory
2D Cloned Git repo
Atype:dropdown
Answer:1A,2C
Container group
Empty directory
Container group is the only logical answer that can have shared lifecycle

https://learn.microsoft.com/en-us/azure/container-instances/container-instances-container-groups?source=recommendations#what-is-a-container-group

Azure files need root permission
https://learn.microsoft.com/en-us/azure/container-instances/container-instances-volume-azure-files

Secret is for secrets and read-only
EmtyDir can persist through crash and redeployed on stop and restart

https://learn.microsoft.com/en-us/azure/container-instances/container-instances-volume-emptydir#emptydir-volume

Cloned Git Repo also does the job but it needs more details like Git URL and stuff which are not mentioned to be available in the question
***********************
N:27
T
You are implementing a software as a service (SaaS) ASP.NET Core web service that will run as an Azure Web App. The web service will use an on-premises
SQL Server database for storage. The web service also includes a WebJob that processes data updates. Four customers will use the web service.
(*) Each instance of the WebJob processes data for a single customer and must run as a singleton instance.
(*) Each deployment must be tested by using deployment slots prior to serving production data.
(*) Azure costs must be minimized.
(*) Azure resources must be located in an isolated network.
You need to configure the App Service plan for the Web App.
How should you configure the App Service plan?
App service plan setting
Number  of VM instances dropdown1
Pricing tier dropdown2
1A 2
1B 4
1C 8
1D 16
2A Isolated
2B Standard
2C Premium
2D Consumption
Atype:dropdown
Answer:1B,2A
Box 1: 4
There are four customers that use this service, and each instance of the WebJob processes data for a single customer and must run as a singleton instance. So, the number of VM should be 4. WebJobs is a feature of Azure App Service that enables you to run a program or script in the same instance as a web app. Like running background tasks.

Box 2: Isolated
Azure resources must be located in an isolated network .
In the Isolated tier, the App Service Environment defines the number of isolated workers that run your apps, and each worker is charged. In addition, there's a flat Stamp Fee for the running the App Service Environment itself. Isolated: This tier runs dedicated Azure VMs on dedicated Azure Virtual Networks. It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities.

https://azure.microsoft.com/sv-se/blog/announcing-app-service-isolated-more-power-scale-and-ease-of-use
***********************
N:28
T
You are creating a CLI script that creates an Azure web app and related services in Azure App Service. The web app uses the following variables:
$gitrepo = "https://github.com.Contos/webapp"
$webappname = "Webapp1103"

You need to automatically deploy code from GitHub to the newly created web app.
How should you complete the script?

az group create --location westeurope --name myResourceGroup
dropdown1 --name $webappname --g myResourceGroup --sku FREE
dropdown2 --name $webappname --g myResourceGroup
dropdown3
dropdown4 source config --name $webappname
--resource-group myResourceGroup dropdown5
1A az webapp
1B az appservice plan create
1C az webapp deployment
1D az group delete
2A az webapp create
2B az appservice plan create
2C az webapp deployment
2D az group delete
3A --repo-url $gitrepo --branch master --manual-integration
3B git clone $gitrepo
3C --plan $webappname
4A az webapp
4B az appservice plan create
4C az webapp deployment
4D az group delete
5A --repo-url $gitrepo --branch master --manual-integration
5B git clone $gitrepo
5C --plan $webappname
Atype:dropdown
Answer:1B,2A,3C,4C,5A
# Replace the following URL with a public GitHub repo URL
gitrepo=https://github.com/Azure-Samples/php-docs-hello-world
webappname=mywebapp$RANDOM

# Create a resource group.
az group create --location westeurope --name myResourceGroup

# Create an App Service plan in `FREE` tier.
az appservice plan create --name $webappname --resource-group myResourceGroup --sku FREE

# Create a web app.
az webapp create --name $webappname --resource-group myResourceGroup --plan $webappname

# Deploy code from a public GitHub repository.
az webapp deployment source config --name $webappname --resource-group myResourceGroup \
--repo-url $gitrepo --branch master --manual-integration

# Copy the result of the following command into a browser to see the web app.
echo http://$webappname.azurewebsites.net
***********************
N:29
T
You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure
Storage Blob storage. The storage account type is General-purpose V2.
When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.
You need to design the process that starts the photo processing.
Which solution/s) are correct?
1 Trigger the photo processing from Blob storage events.
2 Move photo processing to an Azure Function triggered from the blob upload. (question 19 page 6)
3 Convert the Azure Storage account to a BlockBlobStorage storage account.
4: Create an Azure Function app that uses the Consumption hosting model and that is triggered from the blob upload.
5: Use the Azure Blob Storage change feed to trigger photo processing.
ATYPE:MULTISELECT
ANSWER:1,2
You need to catch the triggered event, so move the photo processing to an Azure Function triggered from the blob upload.
Note: Azure Storage events allow applications to react to events. Common Blob storage event scenarios include image or video processing, search indexing, or any file-oriented workflow.
Events are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener.
However, the processing must start in less than one minute.
Note: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage support event integration. Storage (general purpose v1) does not support integration with Event Grid.

https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview

Use Event Grid instead of the Blob storage trigger for the following scenarios:"
1-Blob-only storage accounts: Blob-only storage accounts are supported for blob input and output bindings but not for blob triggers.
2-High-scale: High scale can be loosely defined as containers that have more than 100,000 blobs in them or storage accounts that have more than 100 blob updates per second.
3-Minimizing latency: If your function app is on the Consumption plan, there can be up to a ##10-minute delay in processing new blobs## if a function app has gone idle. To avoid this latency, you can switch to an App Service plan with Always On enabled. You can also use an Event Grid trigger with your Blob storage account. For an example, see the Event Grid tutorial.

https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp#event-grid-trigger

solution 2 is correct but it should have also mentioned that the function app must not be on a consumption plan.

solution 3 is incorrect - the Even Grid integration should be used.
A BlockBlobStorage account is a specialized storage account in the premium performance tier for storing unstructured object data as block blobs or append blobs. Compared with general-purpose v2 and BlobStorage accounts, BlockBlobStorage accounts provide low, consistent latency and higher transaction rates.

https://docs.microsoft.com/en-us/azure/event-grid/resize-images-on-storage-blob-upload-event
https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-performance-tiers

solution 4 is not corrent. Consumption plan can take up to several minutes to trigger the function. See note from https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function.
"When your function app runs in the default Consumption plan, there may be a delay of up to several minutes between the blob being added or updated and the function being triggered. If you need low latency in your blob triggered functions, consider running your function app in an App Service plan."

solution 5 is not corrent, Change feed publishes records to the log within few minutes of the change. The process to produce a mobile-friendly version of the image must start in less than one minute.

https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal
***********************
N:30
T
You develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot.
You need to ensure that scripts run and resources are available before a swap operation occurs.
Which solution(s) are correct?
1 Update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts.
2 Enable auto swap for the Testing slot. Deploy the app to the Testing slot.
3 Disable auto swap. Update the app with a method named statuscheck to run the scripts. Re-enable auto swap and deploy the app to the Production slot.
ATYPE:MULTISELECT
ANSWER:0
Solution 1 is most likely No. The anwser might be right, but the question is wrong. Auto-swap needs to be set on the source slot, not the target slot (production in this question)
https://learn.microsoft.com/en-us/azure/app-service/deploy-staging-slots#configure-auto-swap
https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-swaps
https://learn.microsoft.com/en-us/answers/questions/980267/what-do-we-need-to-warmup-before-swapping-apps-ser

Solution 2 is NO. because if you need scripts to run prior then you need to specify a custom warm up.
because correct solution is updating the web.config file to include applicationinitialization configuration element.
You cannot ensure that scripts run and resources are available before a swap operation occurs by configuring auto swap. The applicationInitialization configuration in the web.config does.

https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#specify-custom-warm-up
Solution 3 is NO. web.config file with applicationinitialization is correct answer. autoswap has to be configured on the Staging Slot.
***********************
N:31
T
You are developing a Docker/Go using Azure App Service Web App for Containers. You plan to run the container in an App Service on Linux.
You identify a Docker container image to use.
None of your current resource groups reside in a location that supports Linux. You must minimize the number of resource groups required.
You need to create the application and perform an initial deployment.
Which three Azure CLI commands should you use to develop the solution?
1 az group create
2 az group update
3 az webapp update
4 az webapp create
5 az appservice plan create
ATYPE:MULTISELECT
ANSWER:1,5,4
az group create
az appservice plan create
az webapp create

https://docs.microsoft.com/en-us/dotnet/architecture/devops-for-aspnet-developers/deploying-to-app-service
***********************
N:32
T
You develop a website. You plan to host the website in Azure. You expect the website to experience high traffic volumes after it is published.
You must ensure that the website remains available and responsive while minimizing cost.
You need to deploy the website.
What should you do?
1 Deploy the website to a virtual machine. Configure the virtual machine to automatically scale when the CPU load is high.
2 Deploy the website to an App Service that uses the Shared service tier. Configure the App Service plan to automatically scale when the CPU load is high.
3 Deploy the website to a virtual machine. Configure a Scale Set to increase the virtual machine instance count when the CPU load is high.
4 Deploy the website to an App Service that uses the Standard service tier. Configure the App Service plan to automatically scale when the CPU load is high.
ATYPE:SINGLESELECT
ANSWER:4
Answer is D.
To ensure that the website remains available and responsive while minimizing cost, you should deploy the website to an App Service that uses the Standard service tier and configure the App Service plan to automatically scale when the CPU load is high. This way, the website can handle high traffic volumes by automatically scaling the number of instances of the website, reducing the risk of the website becoming unavailable due to high traffic.
***********************
N:33
T
A company is developing a Java web app. The web app code is hosted in a GitHub repository located at https://github.com/Contoso/webapp.
The web app must be evaluated before it is moved to production. You must deploy the initial code release to a deployment slot named staging.
You need to create the web app and deploy the code.
How should you complete the commands?

gitrepo=https://github.comcontso/webapp
webappname=businesswebapp
group=businessAppResourceGroup

dropdown1
                        create -l centralus --name $group
dropdown2
                        create --name $webappname -g $group --sku S3
dropdown3
                        create --name $webappname -g $group -plan $webappname
dropdown4
                        create --name $webappname -g $group -slot staging
dropdown5
                        config --name $webappname -g $group --slot staging --repo-url $gitrepo --branch master --manual-integration
1A az group
1B az webapp
1C az appservice plan
1D az webapp deployment slot
1E az webapp deployment source
2A az group
2B az webapp
2C az appservice plan
2D az webapp deployment slot
2E az webapp deployment source
3A az group
3B az webapp
3C az appservice plan
3D az webapp deployment slot
3E az webapp deployment source
4A az group
4B az webapp
4C az appservice plan
4D az webapp deployment slot
4E az webapp deployment source
5A az group
5B az webapp
5C az appservice plan
5D az webapp deployment slot
5E az webapp deployment source
Atype:dropdown
Answer:1A,2C,3B,4D,5E
1 az group
2 az appservice plan
3 az webapp
4 az webapp deployment slot
5 az webapp deployment source

https://docs.microsoft.com/en-us/azure/app-service/scripts/cli-deploy-staging-environment
***********************
N:34
T
You have a web service that is used to pay for food deliveries. The web service uses Azure Cosmos DB as the data store.
You plan to add a new feature that allows users to set a tip amount. The new feature requires that a property named tip on the document in Cosmos DB must be present and contain a numeric value.
There are many existing websites and mobile apps that use the web service that will not be updated to set the tip property for some time.
How should you complete the trigger?

dropdown1
                        create -l centralus --name $group
dropdown2
                        create --name $webappname -g $group --sku S3
dropdown3
                        create --name $webappname -g $group -plan $webappname
dropdown4
                        create --name $webappname -g $group -slot staging
dropdown5
                        config --name $webappname -g $group --slot staging --repo-url $gitrepo --branch master --manual-integration
1A az group
1B az webapp
1C az appservice plan
1D az webapp deployment slot
2A az group
2B az webapp
2C az appservice plan
2D az webapp deployment slot
3A az group
3B az webapp
3C az appservice plan
3D az webapp deployment slot
Atype:dropdown
Answer:1A,2C,3B,4D,5E
1 az group
2 az appservice plan
3 az webapp
4 az webapp deployment slot
5 az webapp deployment source

https://docs.microsoft.com/en-us/azure/app-service/scripts/cli-deploy-staging-environment
***********************
N:35
T
You have a web service that is used to pay for food deliveries. The web service uses Azure Cosmos DB as the data store.
You plan to add a new feature that allows users to set a tip amount. The new feature requires that a property named tip on the document in Cosmos DB must be present and contain a numeric value.
There are many existing websites and mobile apps that use the web service that will not be updated to set the tip property for some time.
How should you complete the trigger?

function ensureTip() {
var r = dropdown1
var i = r.getBody();
dropdown2
     i["tip"] = 0;
dropdown3
1A getContext().value();
1B getContext().readDocument('item');
1C getContext().getRequest();
1D getContext().getResponse();
2A if (!("tip" in i )) {
2B if (request.getValue("tip") === null) {
2C if (isNaN(i)["tip"] || i["tip"] === null ) {
2D if ( typeof i.pluck("tip") === null ) {
3A r.setBody(i);
3B r.setValue(i);
3C r.upsetDocument(i);
3D r.replaceDocument(i);
Atype:dropdown
Answer:1C,2A,3A
getContext().getRequest();
if (!("tip" in i )) {
r.setBody(i);

https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-write-stored-procedures-triggers-udfs
***********************
N:36
T
You develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob.
The app continues to time out after four minutes. The app must process the blob data.
You need to ensure the app does not time out and processes the blob data.
Which solution(s)?
1 Use the Durable Function async pattern to process the blob data.
2 Pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response.
3 Configure the app to use an App Service hosting plan and enable the Always On setting.
4 Update the functionTimeout property of the host.json project file to 10 minutes.
ATYPE:MULTISELECT
ANSWER:1,2
Solution 1 is correct. For longer processing times, consider using the DURABLE FUNCTIONS ASYNC PATTERN.
Solution 2 is correct, though durable functions would be better.
Solution 3 is NO. the timeout being raised by HTTP layer from the Azure Load Balancer, not the App layer that at least it gives 5 minutes for the cheapest type, Consumption, so however you enhance the app layer, the http layer Azure Load Balance will not wait more than 230 second and will reply it as timeout. Use the durable function pattern to poll the status for completion will be the easiest solution, else avoid the http layer like service bus will work too.
Solution 4 is NO. Regardless of the function app timeout setting, 230 seconds is the maximum amount of time that an HTTP triggered function can take to respond to a request.
https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale#timeout
Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response.
Note: Large, long-running functions can cause unexpected timeout issues. General best practices include:
Whenever possible, refactor large functions into smaller function sets that work together and return responses fast. For example, a webhook or HTTP trigger function might require an acknowledgment response within a certain time limit; it's common for webhooks to require an immediate response. You can pass the
HTTP trigger payload into a queue to be processed by a queue trigger function. This approach lets you defer the actual work and return an immediate response.
Reference:
https://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices

Always On enables waking up on HTTPTrigger, but does not prevent the exceeding the max time out time of 230 seconds.
https://docs.microsoft.com/en-us/azure/azure-functions/dedicated-plan#always-on
https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#timeout
https://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices
https://docs.microsoft.com/en-us/azure/azure-functions/performance-reliability#avoid-long-running-functions
***********************
N:37
T
You are developing an application that uses Azure Blob storage.
The application must read the transaction logs of all the changes that occur to the blobs and the blob metadata in the storage account for auditing purposes. The changes must be in the order in which they occurred, include only create, update, delete, and copy operations and be retained for compliance reasons.
You need to process the transaction logs asynchronously.
What should you do?
1 Process all Azure Blob storage events by using Azure Event Grid with a subscriber Azure Function app.
2 Enable the change feed on the storage account and process all changes for available events.
3 Process all Azure Storage Analytics logs for successful blob events.
4 Use the Azure Monitor HTTP Data Collector API and scan the request body for successful blob events.
ATYPE:SINGLESELECT
ANSWER:2
The purpose of the change feed is to provide transaction logs of all the changes that occur to the blobs and the blob metadata in your storage account. The change feed provides ordered, guaranteed, durable, immutable, read-only log of these changes. Client applications can read these logs at any time, either in streaming or in batch mode. The change feed enables you to build efficient and scalable solutions that process change events that occur in your Blob Storage account at a low cost.
https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed
***********************
N:38
T
You plan to create a Docker image that runs an ASP.NET Core application named ContosoApp. You have a setup script named setupScript.ps1 and a series of application files including ContosoApp.dll.
You need to create a Dockerfile document that meets the following requirements:
* Call setupScripts.ps1 when the container is built.
* Run ContosoApp.dll when the container starts.
The Dockerfile document must be created in the same folder where ContosoApp.dll and setupScript.ps1 are stored.
Which five commands should you use to develop the solution? To answer, move the appropriate commands from the list of commands to the answer area and arrange them in the correct order.
group-1-start
CMD ["dotnet", "ContsoApp.dll"]
group-1-end
group-2-start
FROM microsoft/aspnetcore:latest
group-2-end
group-3-start
COPY ./ .
group-3-end
group-4-start
RUN powershell ./setupScript.ps1
group-4-end
group-5-start
WORKDIR /apps/ContsoApp
group-5-end
ATYPE:ORDER
ANSWER:2,5,3,4,1
The first statement in the Dockefile must be the FROM statement to specify the image to use as the base image.
Then specify the Image working directory
Then copy all of the application contents using the COPY command
And then use the CMD command to run the PowerShell command and the ENTRYPOINT statement to run the dotnet application.
=> Call setupScripts.ps1 when the container is built. ( so definitely RUN comes first)
=> Run ContosoApp.dll when the container starts. (Therefore CMD comes next)
***********************
N:39
T
You are developing an Azure Function App that processes images that are uploaded to an Azure Blob container.
Images must be processed as quickly as possible after they are uploaded, and the solution must minimize latency. You create code to process images when the
Function App is triggered.
You need to configure the Function App.
What should you do?
1 Use an App Service plan. Configure the Function App to use an Azure Blob Storage input trigger.
2 Use a Consumption plan. Configure the Function App to use an Azure Blob Storage trigger.
3 Use a Consumption plan. Configure the Function App to use a Timer trigger.
4 Use an App Service plan. Configure the Function App to use an Azure Blob Storage trigger.
5 Use a Consumption plan. Configure the Function App to use an Azure Blob Storage input trigger.
ATYPE:SINGLESELECT
ANSWER:4
The answer is D. Use an App Service plan. Configure the Function App to use an Azure Blob Storage trigger.
Consumption plan can cause a 10-min delay in processing new blobs if a function app has gone idle. To avoid this latency, you can switch to an App Service plan with Always On enabled.
We have to use an Azure Blob storage trigger. In order to ensure the function is invoked immediately.

https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp
***********************
N:40
T
You are configuring a new development environment for a Java application.
The environment requires a Virtual Machine Scale Set (VMSS), several storage accounts, and networking components.
The VMSS must not be created until the storage accounts have been successfully created and an associated load balancer and virtual network is configured.
How should you complete the Azure Resource Manager template? To answer, select the appropriate options in the answer area.

{
....
"resources": [
{
 "apiVersion": "2016-01-01", 
 "type": "Microsoft.Storage/StorageAccounts", 
 "name": "[concat(dropdown1, 'storage', uniqueString(resourceGroup().id))]", 
 "location": "[resourceGroup().location]",
...
 "sku": {
  "name": "Standard_LRS"
 },
 "kind": "Storage", 
 "properties": {}, 
 "[dropdown2]": {
 "name": "StorageSetup",
 "count": 3
}
},
{
 "apiVersion": "2015-06-15",
 "type": "Micorsoft.Compute/virtualMachines",
 "name": "[concat('VM', uniqueString(resourceGroup().id))]",
 "[dropdown3]": [
 "[variables('loadBalancerName')]"
 "[variables('virtualNetworkName')]"
"storagesetup", 
],
..
}] "outputs": {}
1A copy()
1B copyIndex()
1C priority()
1D dependsOn()
2A copy
2B copyIndex
2C priority
2D dependsOn
3A copy
3B copyIndex
3C priority
3D dependsOn
Atype:dropdown
Answer:1B,2A,3D
Box 1: copyIndex
Notice that the name of each resource includes the copyIndex() function, which returns the current iteration in the loop. copyIndex() is zero-based.

Box 2: copy
By adding copy loop to the resources section of your template, you can dynamically set the number of resources to deploy. You also avoid having to repeat template syntax.

Box 3: dependsOn
Within your Azure Resource Manager template (ARM template), the dependsOn element enables you to define one resource as a dependent on one or more resources.

Reference:
https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/copy-resources
https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/quick-create-template-windows
https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/define-resource-dependency
***********************
N:41
T
You are developing an Azure Function App by using Visual Studio. The app will process orders input by an Azure Web App. The web app places the order information into Azure Queue Storage.
You need to review the Azure Function App code shown below.

public static void ProcessOrders([QueueTrigger("incoming-orders")]CloudQueueMessage item, [Table("Orders")]ICollector<Order> tableBindings, TraceWriter log)
{
  log.Info($"processing order: {item.Id}");
  log.Info($"queue insertion time {item.InsertionTime}");
  log.Info($"queue expiration time {item.ExpirationTime}");
 table.Bindings.Add(JsonConvert.DeserializeObject<Order>(item.AsString) );
}
[FunctionName("Process-Orders-Poison")]
public static void ProcessFailedOrders([QueueTrigger("incoming-orders-poison")]CloudQueueMessage item, TraceWriter log )
{
  log.Error($"Failed to process order: {item.AsString} ");
...
}

1 The code will log the time that the order was process from the queue?
2 When the ProcessOrders function fails, the funtion will retry up to five times for a given order, including the first try?
3 When there are multiple orders in the queue, a batch of orders will be retrived from the queue and the ProcessOrders-function will run multiple instances concurrently to process the orders?
4 The ProcessOrders-function will output the order to an Orders table in Azure Table Storage?
ATYPE:MULTISELECT
ANSWER:2,3,4
Box 1: No
It logs the following:
- ExpirationTime - The time that the message expires.
- InsertionTime - The time that the message was added to the queue.

Box 2: Yes
maxDequeueCount: The number of times to try processing a message before moving it to the poison queue. Default value is 5.

Box 3: Yes
When there are multiple queue messages waiting, the queue trigger retrieves a batch of messages and invokes function instances concurrently to process them. By default, the batch size is 16. When the number being processed gets down to 8, the runtime gets another batch and starts processing those messages. So the maximum number of concurrent messages being processed per function on one virtual machine (VM) is 24.

Box 4: Yes
[Table("Orders")]ICollector<Order> table bindings
And in the code it adds the order:
tableBindings.Add(JsonConvert.DeserializeObject<Object>(myQueueItem.AsString));

https://docs.microsoft.com/fr-fr/azure/azure-functions/functions-bindings-storage-queue#hostjson-settings
***********************
***********************
N:42
T
You are developing a solution for a hospital to support the following use cases:
(*) The most recent patient status details must be retrieved even if multiple users in different locations have updated the patient record.
(*) Patient health monitoring data retrieved must be the current version or the prior version.
(*) After a patient is discharged and all charges have been assessed, the patient billing record contains the final charges.
You provision a Cosmos DB NoSQL database and set the default consistency level for the database account to Strong. You set the value for Indexing Mode to Consistent.
You need to minimize latency and any impact to the availability of the solution. You must override the default consistency level at the query level to meet the required consistency guarantees for the scenarios.
Which consistency levels should you implement?

Return the most recent patient status. => dropdown1
Return healt monitoring data that is no less than one version behind. => dropdown2
After patient is discharched and all charges are assessed, retrieve the correct billing data with the final charges. => dropdown3
1A Consistancy levels
1B Strong
1C Consistent Prefix
1D Bounded Staleness
1E Eventual
2A Consistancy levels
2B Strong
2C Consistent Prefix
2D Bounded Staleness
2E Eventual
3A Consistancy levels
3B Strong
3C Consistent Prefix
3D Bounded Staleness
3E Eventual
Atype:dropdown
Answer:1B,2D,2E
Box 1: Strong -
Strong: Strong consistency offers a linearizability guarantee. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.

Box 2: Bounded staleness -
Bounded staleness: The reads are guaranteed to honor the consistent-prefix guarantee. The reads might lag behind writes by at most "K" versions (that is
"updates") of an item or by "t" time interval. When you choose bounded staleness, the "staleness" can be configured in two ways:
The number of versions (K) of the item
The time interval (t) by which the reads might lag behind the writes

Box 3: Eventual -
Eventual: There's no ordering guarantee for reads. In the absence of any further writes, the replicas eventually converge.
Incorrect Answers:
Consistent prefix: Updates that are returned contain some prefix of all the updates, with no gaps. Consistent prefix guarantees that reads never see out-of-order writes.

https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels
***********************
N:43
T
You are configuring a development environment for your team. You deploy the latest Visual Studio image from the Azure Marketplace to your Azure subscription.
The development environment requires several software development kits (SDKs) and third-party components to support application development across the organization. You install and customize the deployed virtual machine (VM) for your development team. The customized VM must be saved to allow provisioning of a new team member development environment.
You need to save the customized VM for future provisioning.
Which tools or services should you use?
1 [Generalize the VM]
2 Azure PowerShell
3 Visual Studio command prompt
4 Azure Migate
5 Azure Backup
6 [Store images]
7 Azure Blob Storage
8 Azure Data Lake Storage
9 Azure File Storage
10 Azure Table Storage
ATYPE:MULTISELECT
ANSWER:2,7
Box 1: Azure Powershell
Creating an image directly from the VM ensures that the image includes all of the disks associated with the VM, including the OS disk and any data disks. Before you begin, make sure that you have the latest version of the Azure PowerShell module. You use Sysprep to generalize the virtual machine, then use Azure PowerShell to create the image.

Box 2: Azure Blob Storage
A VM Image is a collection of metadata and pointers to a set of VHDs (one VHD per disk) stored as page blobs in Azure Storage.

https://azure.microsoft.com/en-us/blog/vm-image-blog-post

https://docs.microsoft.com/en-us/azure/virtual-machines/windows/capture-image-resource

https://learn.microsoft.com/en-us/azure/virtual-machines/generalize
***********************
N:44
T
You are preparing to deploy a website to an Azure Web App from a GitHub repository. The website includes static content generated by a script.
You plan to use the Azure Web App continuous deployment feature.
You need to run the static generation script before the website starts serving traffic.
What are two possible ways to achieve this goal?

1 Add the path to the static content generation tool to WEBSITE_RUN_FROM_PACKAGE setting in the host.json file.
2 Add a PreBuild target in the websites csproj project file that runs the static content generation script.
3 Create a file named run.cmd in the folder /run that calls a script which generates the static content and deploys the website.
4 Create a file named .deployment in the root of the repository that calls a script which generates the static content and deploys the website.
ATYPE:MULTISELECT
ANSWER:2,4
Correct answer is B and D
- run.cmd is used only to start a project as a dll file https://www.sohaibtariq.com/console-webjob/%C3%ACndex/

- WEBSITE_RUN_FROM_PACKAGE doesn't consent the execution of any script. You can only run your web project from a package (.zip file tipically)
https://docs.microsoft.com/bs-latn-ba/azure/azure-functions/run-functions-from-deployment-package
https://github.com/Azure/app-service-announcements/issues/84

- Instead in .csproj file PreBuild Event Target you can specify any command to execute before the compilation and the application execution.

Could be C instead of B => To customize your deployment, include a .deployment file in the repository root. For more information, see Customize deployments and Custom deployment script.

https://docs.microsoft.com/en-us/azure/app-service/deploy-continuous-deployment?tabs=github
https://github.com/projectkudu/kudu/wiki/Custom-Deployment-Script


https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild-targets?view=vs-2019
https://stackoverflow.com/questions/44818730/is-there-a-net-core-cli-pre-before-build-task
https://stackoverflow.com/questions/28916414/visual-studio-add-pre-build-event-that-always-runs-c-project
https://docs.microsoft.com/en-us/azure/app-service/deploy-best-practices#net
***********************
N:45
T
You are developing an application to use Azure Blob storage. You have configured Azure Blob storage to include change feeds.
A copy of your storage account must be created in another region. Data must be copied from the current storage account to the new storage account directly between the storage servers.
You need to create a copy of the storage account in another region and copy the data.
In which order should you perform the actions?
group-1-start
Use AZCopy to copy the data to the new storage account
group-1-end
group-2-start
Deploy the template to create a new storage account in the target region.
group-2-end
group-3-start
Export a Resource Manager template.
group-3-end
group-4-start
Create a new template deployment.
group-4-end
group-5-start
Modify the template by changing the storage name and region.
group-5-end
ATYPE:ORDER
ANSWER:3,4,5,2,1
Export -
Create New Template Deployment
Modify
Deploy
AzCopy

https://docs.microsoft.com/en-us/azure/storage/common/storage-account-move?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&tabs=azure-portal
***********************
N:46
T
You are preparing to deploy an Azure virtual machine (VM)-based application.
The VMs that run the application have the following requirements:
(*) When a VM is provisioned the firewall must be automatically configured before it can access Azure resources.
(*) Supporting services must be installed by using an Azure PowerShell script that is stored in Azure Storage.
You need to ensure that the requirements are met.
Which features should you use?

Firewall configuration => dropdown1
Supporting services script => dropdown2
1A Run Command
1B Serial console
1C Hybrid Runbook Worker
1D Custom Script Extension
2A Run Command
2B Serial console
2C Hybrid Runbook Worker
2D Custom Script Extension
Atype:dropdown
Answer:1A,2D
Box 1: Run Command
This capability is useful in all scenarios where you want to run a script within a VM. It's one of the only ways to troubleshoot and remediate a VM that doesn't have the RDP or SSH port open, because of improper network or administrative user configuration.

Box 2: Customer Script Extension
The Custom Script Extension downloads and executes scripts on Azure virtual machines. This extension is useful for post deployment configuration, software installation, or any other configuration or management tasks. Scripts can be downloaded from Azure storage or GitHub, or provided to the Azure portal at extension run time. The Custom Script Extension integrates with Azure Resource Manager templates, and can be run using the Azure CLI, PowerShell, Azure portal, or the Azure Virtual Machine REST API.

https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/custom-script-windows
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/run-scripts-in-vm
***********************
N:47
T
A company is developing a Node.js web app. The web app code is hosted in a GitHub repository located at https://github.com/TailSpinToys/webapp.
The web app must be reviewed before it is moved to production. You must deploy the initial code release to a deployment slot named review.
You need to create the web app and deploy the code.

dropdown1 -Name $group -Location $location
dropdown2 -Name $webappname -Location $location -ResourceGroupName $group -Tier Standard
dropdown3 -Name $webappname -Location $location -AppServicePlan $webappname -ResourceGroupName $group -Tier Standard
dropdown4 -Name $webappname -ResourceGroupName $group -Slot review
1A New-AzWebAppSlot
1B New-AzWebApp
1C New-AzWebServicePlan
1D New-AzResourceGroup
2A New-AzWebAppSlot
2B New-AzWebApp
2C New-AzWebServicePlan
2D New-AzResourceGroup
3A New-AzWebAppSlot
3B New-AzWebApp
3C New-AzWebServicePlan
3D New-AzResourceGroup
4A New-AzWebAppSlot
4B New-AzWebApp
4C New-AzWebServicePlan
4D New-AzResourceGroup
Atype:dropdown
Answer:1D,2C,3B,4A
Box 1: New-AzResourceGroup
The New-AzResourceGroup cmdlet creates an Azure resource group.

Box 2: New-AzAppServicePlan
The New-AzAppServicePlan cmdlet creates an Azure App Service plan in a given location

Box 3: New-AzWebApp
The New-AzWebApp cmdlet creates an Azure Web App in a given a resource group

Box 4: New-AzWebAppSlot
The New-AzWebAppSlot cmdlet creates an Azure Web App slot.

https://docs.microsoft.com/en-us/powershell/module/az.resources/new-azresourcegroup?view=azps-2.3.2
https://docs.microsoft.com/en-us/powershell/module/az.websites/new-azappserviceplan?view=azps-2.3.2
https://docs.microsoft.com/en-us/powershell/module/az.websites/new-azwebapp?view=azps-2.3.2
https://docs.microsoft.com/en-us/powershell/module/az.websites/new-azwebappslot?view=azps-2.3.2
***********************
N:48
T
You are developing an application that needs access to an Azure virtual machine (VM).
The access lifecycle for the application must be associated with the VM service instance.
You need to enable managed identity for the VM.
How should you complete the PowerShell segment?

$vm = Get-AzVM -ResourceGroupName "ContosoRG" -Name "ContosoVM"
Update-AzVM -ResourceGroupName "ContosoRG" -VM $vm dropdown1 dropdown2
1A -AssignIdentity:
1B -IdentityType:
2A SystemAssigned
2B $UserAssigned
Atype:dropdown
Answer:1B,2A
Box 1: -IdentityType
-IdentityType: The type of identity used for the virtual machine. Valid values are SystemAssigned, UserAssigned, SystemAssignedUserAssigned, and None.

-IdentityId: Specifies the list of user identities associated with the virtual machine. The user identity references will be ARM resource IDs in the form:

Box 2: $SystemAssigned
There are two types of managed identities:
- System-assigned: Some Azure services allow you to enable a managed identity directly on a service instance. When you enable a system-assigned managed identity an identity is created in Azure AD that is tied to the lifecycle of that service instance. So when the resource is deleted, Azure automatically deletes the identity for you. By design, only that Azure resource can use this identity to request tokens from Azure AD.
- User-assigned: You may also create a managed identity as a standalone Azure resource. You can create a user-assigned managed identity and assign it to one or more instances of an Azure service. In the case of user-assigned managed identities, the identity is managed separately from the resources that use it.

Reference:
https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/qs-configure-powershell-windows-vm

https://docs.microsoft.com/en-us/powershell/module/az.compute/update-azvm?view=azps-5.8.0&viewFallbackFrom=azps-5.6.0
https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.management.compute.models.resourceidentitytype?view=azure-dotnet
***********************
N:49
T
You develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot.
You need to ensure that scripts run and resources are available before a swap operation occurs.
Solution: Update the app with a method named statuscheck to run the scripts. Update the app settings for the app. Set the WEBSITE_SWAP_WARMUP_PING_PATH and WEBSITE_SWAP_WARMUP_PING_STATUSES with a path to the new method and appropriate response codes.
Does the solution meet the goal?
ATYPE:STATEMENT
ANSWER:YES
Yes
You can also customize the warm-up behavior with one or both of the following app settings:

WEBSITE_SWAP_WARMUP_PING_PATH: The path to ping to warm up your site. Add this app setting by specifying a custom path that begins with a slash as the value. An example is /statuscheck. The default value is /.
WEBSITE_SWAP_WARMUP_PING_STATUSES: Valid HTTP response codes for the warm-up operation. Add this app setting with a comma-separated list of HTTP codes. An example is 200,202 . If the returned status code isn't in the list, the warmup and swap operations are stopped. By default, all response codes are valid.
WEBSITE_WARMUP_PATH: A relative path on the site that should be pinged whenever the site restarts (not only during slot swaps). Example values include /statuscheck or the root path, /.

https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots
***********************
N:50
T
You create the following PowerShell script:

$Source = New-AzScheduledQueueRuleSource -Query 'Heartbeat | where TimeGenerated > ago(1h)' -DataSourceId "contoso"
$schedule = New-AzScheduledQueryRuleSchedule -FrequencyInMinutes 60 -TimeWindowInMinutes 60
$triggerCondition = New-AzScheduledQueryRuleTriggerCondition -ThresholdOperator "LessThen" -Threshold 5
$aznsActionGroup = New-AzScheduledQueryRuleAznsActionGroup -ActionGroup "contoso" -EmailSubject "Custom email subject"
  -CustomWebhookPayload "{ '"alert'":'"#alertrulename'", '"IncludeSearchResults'": true }"
$alertingAction = New-AzScheduledQueryRuleAlertingAction -AznsAction $aznsActionGroup -Severity "3" -Trigger $triggerCondition
New-AzScheduledQueryRule -ResourceGroupName "contoso" -Location "eastus" -Action $alertingAction -Enabled $true
  -Description "Alert description" -Schedule $schedule -Source $source -Name "Alert Name"

1 A log alert is created that sends an email when the CPU percentage is above 60 percenty for 5 minutes
2 A log alert is created that sends an email when the number of the virtual machine heartbeats in the past hour is less that 5
3 The log alert is scheduled to run every two hours
ATYPE:MULTISELECT
ANSWER:2
Box 1: No -
The AzScheduledQueryRuleSource is Heartbeat, not CPU.

Box 2: Yes -
The AzScheduledQueryRuleSource is Heartbeat!
Note: New-AzScheduledQueryRuleTriggerCondition creates an object of type Trigger Condition. This object is to be passed to the command that creates Alerting
Action object.

Box 3: No -
The schedule is 60 minutes, not two hours.
-FrequencyInMinutes: The alert frequency.
-TimeWindowInMinutes: The alert time window
The New-AzAscheduledQueryRuleSchedule command creates an object of type Schedule. This object is to be passed to the command that creates Log Alert
Rule.

https://docs.microsoft.com/en-us/powershell/module/az.monitor/new-azscheduledqueryrule https://docs.microsoft.com/en-us/powershell/module/az.monitor/new-azscheduledqueryruletriggercondition

Commands will send an email
Where threshold is < 5
Every 60 mins
Where heartbeat is > 1h ago

https://learn.microsoft.com/en-us/javascript/api/@azure/arm-monitor/schedule?view=azure-node-latest
***********************
N:51
T
You are developing an Azure Function app.
The app must meet the following requirements:
(*) Enable developers to write the functions by using the Rust language.
(*) Declaratively connect to an Azure Blob Storage account.
You need to implement the app.
Which Azure Function app features should you use?

Requirement => Feature
Enable developers to write the function by using the Rust language. => [dropdown1]
Declaratively connect to an Azure Blob Storage account. => [dropdown2]
1A Custom handler
1B Extention bundle
1C Trigger
1D Runtime
1E Policy
1F Hosting plan
2A Custom handler
2B Extention bundle
2C Trigger
2D Runtime
2E Policy
2F Hosting plan
Atype:dropdown
Answer:1A,2B
Box 1: Custom handler
Custom handlers can be used to create functions in any language or runtime by running an HTTP server process, for example Go or Rust.

Box 2: extension bundles
is needed to support the bindings and triggers that you use
https://docs.microsoft.com/en-us/azure/azure-functions/functions-custom-handlers?WT.mc_id=thomasmaurer-blog-thmaure#bindings-support

https://learn.microsoft.com/en-us/azure/azure-functions/functions-custom-handlers
- With custom handlers, you can use triggers and input and output bindings via extension bundles.

Box 2 is almost Trigger, but then you have only ONE trigger per function. The question says nothing about how the function is being triggered, only that you need to connect to Blob Storage. What if you wanted to trigger the function by HTTP request and then perform some action in Blob Storage? You can't do that with Blob Storage trigger.
***********************
N:52
T
You are developing an ASP.NET Core web application. You plan to deploy the application to Azure Web App for Containers.
The application needs to store runtime diagnostic data that must be persisted across application restarts. You have the following code:

public void SaveDiaData(string data)
{
  var path = Environment.GetEnvironmentVariable("DIAGDATA")
  File.WriteAllText(Path.Combine(path, "data"), data);
}

You need to configure the application settings so that diagnostic data is stored as required.
How should you configure the web app's settings?
dropdown1: dropdown2
DIAGDATA: dropdown3
1A LOCALAPPDATA
1B WEBSITE_LOCALCACHE_ENABLED
1C DOTNET_HOSTING_OPTIMIZTION_CACHE
1D WEBSITES_ENABLE_APP_SERVICE_STORAGE
2A true
2B false
2C local
2D home
2E remote
3A /home
3B /local
3C D:\home
3D D:\local
Atype:dropdown
Answer:1D,2A,3A
WEBSITES_ENABLE_APP_SERVICE_STORAGE=true
DIAGDATA=/home

Reference:
https://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-faq
https://learn.microsoft.com/en-us/azure/app-service/reference-app-settings?tabs=kudu%2Cdotnet#custom-containers
***********************
N:53
T
You are developing a web app that is protected by Azure Web Application Firewall (WAF). All traffic to the web app is routed through an Azure Application
Gateway instance that is used by multiple web apps. The web app address is contoso.azurewebsites.net.
All traffic must be secured with SSL. The Azure Application Gateway instance is used by multiple web apps.
You need to configure the Azure Application Gateway for the web app.
Which two actions should you perform?
1 In the Azure Application Gateway's HTTP setting, enable the Use for App service setting.
2 Convert the web app to run in an Azure App service environment (ASE).
3 Add an authentication certificate for contoso.azurewebsites.net to the Azure Application Gateway.
4 In the Azure Application Gateway's HTTP setting, set the value of the Override backend path option to contoso.azurewebsites.net.
ATYPE:MULTISELECT
ANSWER:1,4
D: The ability to specify a host override is defined in the HTTP settings and can be applied to any back-end pool during rule creation.
The ability to derive the host name from the IP or FQDN of the back-end pool members. HTTP settings also provide an option to dynamically pick the host name from a back-end pool member's FQDN if configured with the option to derive host name from an individual back-end pool member.
A (not C): SSL termination and end to end SSL with multi-tenant services.
In case of end to end SSL, trusted Azure services such as Azure App service web apps do not require whitelisting the backends in the application gateway.
Therefore, there is no need to add any authentication certificates.
https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-web-app-overview

A. yes, app is using default appservice domain azurewebsites.net, so my assumption is, it's hosted as appservice, 
B - it's not needed as it's already running as App Service, therefore C - is not needed as well, 
D - yes you need either pool or redirection, in this case we need to redirect the traffic, there's 100% typo in hostname
Here's an article where the answer D is explained in detail.
https://learn.microsoft.com/en-us/azure/application-gateway/configure-web-app?tabs=customdomain%2Cazure-portal
***********************
N:54
T
You are developing a web application that runs as an Azure Web App. The web application stores data in Azure SQL Database and stores files in an Azure
Storage account. The web application makes HTTP requests to external services as part of normal operations.
The web application is instrumented with Application Insights. The external services are OpenTelemetry compliant.
You need to ensure that the customer ID of the signed in user is associated with all operations throughout the overall system.
What should you do?
1 Add the customer ID for the signed in user to the CorrelationContext in the web application
2 On the current SpanContext, set the TraceId to the customer ID for the signed in user
3 Set the header Ocp-Apim-Trace to the customer ID for the signed in user
4 Create a new SpanContext with the TraceFlags value set to the customer ID for the signed in user
ATYPE:MULTISELECT
ANSWER:1
The CorrelationContext is a way to associate contextual information with a request as it flows through the system. It allows you to track a request as it passes through different components of the system, and to identify related log entries and telemetry data. By adding the customer ID to the CorrelationContext in the web application, you can ensure that it is associated with all operations throughout the overall system. This will allow you to track the request and identify related log entries and telemetry data for a specific customer.

https://docs.microsoft.com/en-us/azure/azure-monitor/app/correlation

https://open-telemetry.github.io/opentelemetry-js-api/interfaces/spancontext.html
https://learn.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-enable?tabs=java#set-the-user-id-or-authenticated-user-id

A SpanContext represents the portion of a Span which must be serialized and propagated along side of a Baggage.
***********************
N:55
T
You are developing an Azure Function App. You develop code by using a language that is not supported by the Azure Function App host. The code language supports HTTP primitives.
You must deploy the code to a production Azure Function App environment.
You need to configure the app for deployment.
Which configuration values should you use?

configuration parameter => configuration value
Publish                 => [dropdown1]
Runtime stack           => [dropdown2]
Version                 => [dropdown3]
1A Code
1B Docker Container
2A Node.js
2B Python
2C Powershell Core
2D Custom handler
3A 14 LTS
3B 7.0
3C custom
Atype:dropdown
Answer:1A,2D,3C
>Code
>Custom Handler
>custom (only option when you pick Custom Handler)
if you pick docker container you cant specify stack and version

https://learn.microsoft.com/en-us/azure/azure-functions/functions-custom-handlers

https://learn.microsoft.com/en-us/azure/azure-functions/supported-languages

https://docs.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-other?tabs=rust%2Cwindows

***********************
N:56
T
You provision virtual machines (VMs) as development environments.
One VM does not start. The VM is stuck in a Windows update process. You attach the OS disk for the affected VM to a recovery VM.
You need to correct the issue.
In which order should you perform the actions?

group-1-start
Run the following command at an elevated command prompt:
dism /image:\ /get=packages > c:\temp\Patch.txt
group-1-end
group-2-start
Run the following command at an elevated command prompt:
dism /Image:<Attached OS disks>:\ /Remove
Package /PackageName:<package name to delete>
group-2-end
group-3-start
Detach the OS disk and recreate the VM
group-3-end
group-4-start
Open c:\temp\Patch.txt file and locale the update that is in a pending state
group-4-end
ATYPE:ORDER
ANSWER:2,1,4,3
1. Take a snapshot of the OS disk of the affected VM as a backup.
2. Attach the OS disk to a recovery VM.
3. Once the OS disk is attached on the recovery VM, run diskmgmt.msc to open Disk Management, and ensure the attached disk is ONLINE.
4. (Step 1) Open an elevated command prompt instance (Run as administrator). Run the following command to get the list of the update packages that are on the attached OS disk: dism /image:<Attached OS disk>:\ /get-packages > c:\temp\Patch_level
5. (Step 2) Open the C:\temp\Patch_level.txt file, and then read it from the bottom up. Locate the update that's in Install Pending or Uninstall Pending state.
6. Remove the update that caused the problem:
dism /Image:<Attached OS disk>:\ /Remove-Package /PackageName:<PACK
7. (Step 4) Detach the OS disk and recreate the VM. Then check whether the issue is resolved.

https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/troubleshoot-stuck-updating-boot-error
***********************
N:57
T
You are developing an Azure Durable Function based application that processes a list of input values. The application is monitored using a console application that retrieves JSON data from an Azure Function diagnostic endpoint.
During processing a single instance of invalid input does not cause the function to fail. Invalid input must be available to the monitoring application.
You need to implement the Azure Durable Function and the monitoring console application.

How should you complete the code segments?

[FunctionName("App")]
public static async Task<List<string>> RunOrchestor(
  [OrchestrationTrigger] IDurableOrchestrationContext context){
    EntityId[] input = ...
int errIndex = ...
dropdown1
using (var client = httpClient())
{
  while(true)
  {
    var response = await client.GetAsync("...");
    response.EnsureSuccessStatusCode();
    var json = await response.Content.ReadAsStringAsync();
    dynamic result = JsonConvert.DeserializeObject(json);
    if(result.runtimeStatus == "dropdown2") 
    {
      return result.dropdown3;
    }
  }
}
1A context.SetOutput(input[errIndex])
1B context.SetCustomStatus(input[errIndex])
1C context.SignalEntity(input[errIndex], "error")
1D await context.CallEntityAsync(input[errIndex], "error")
2A Failed
2B Awaited
2C Listening
2D Completed
3A input
3B output
3C runtimeStatus
3D customStatus
Atype:dropdown
Answer:1D,2C
Box 1: await context.CallEntityAsync(input[errindex],"error")
Orchestration signals and calls an entity
Orchestrator functions can access entities by using APIs on the orchestration trigger binding.
Example:
[FunctionName("CounterOrchestration")]
public static async Task Run(
[OrchestrationTrigger] IDurableOrchestrationContext context)
{
var entityId = new EntityId(nameof(Counter), "myCounter");
// Two-way call to the entity which returns a value - awaits the response int currentValue = await context.CallEntityAsync<int>(entityId, "Get");
It is not signalEntry, Signals an operation to be performed by an entity at a specified time. Any result or exception is ignored (fire and forget).
We're clearly waiting for completion here, so have to use CallEntity,

Box 2: Failed -
During processing a single instance of invalid input does not cause the function to fail.
Note: RuntimeStatus: One of the following values:
Failed: The instance failed with an error.
Completed: The instance has completed normally.
Terminated: The instance was stopped abruptly.
Pending: The instance has been scheduled but has not yet started running.
Running: The instance has started running.
ContinuedAsNew: The instance has restarted itself with a new history. This state is a transient state.

Box 3: Input -
Invalid input must be available to the monitoring application.

https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-entities https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-instance-management
***********************
N:58
T
You are developing an Azure Durable Function to manage an online ordering process.
The process must call an external API to gather product discount information.
You need to implement the Azure Durable Function.
Which Azure Durable Function types should you use? Each correct answer presents part of the solution.
1 Orchestrator
2 Entity
3 Client
4 Activity
ATYPE:MULTISELECT
ANSWER:1,4
Orchestrator and activity is correct.
Activity for this reason "Unlike orchestrator functions, activity functions aren't restricted in the type of work you can do in them. Activity functions are frequently used to make network calls or run CPU intensive operations. An activity function can also return data back to the orchestrator function."
Reference: https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-types-features-overview#activity-functions
Not Entity function, it is for storing state which isn't called out as requirement. 

The "Orchestrator" function type is responsible for coordinating the workflow and managing the execution of the "Activity" function type. It receives inputs, calls external APIs, and makes decisions based on the results of the API calls.
The "Activity" function type is responsible for performing specific actions or tasks, such as calling an external API to gather product discount information. It receives inputs from the "Orchestrator" function and returns results back to it.
***********************
N:59
T
You are authoring a set of nested Azure Resource Manager templates to deploy multiple Azure resources.
The templates must be tested before deployment and must follow recommended practices.
You need to validate and test the templates before deployment.
Which tools should you use?

Determine whether the template follow recommented practices => Tool to use dropdown1
Test and validate changes that template will make to the environment => Tool to use dropdown2
1A Parameter file
1B Template function
1C Azure Reource Manager test toolkit
1D User-defined function
1E What-if operation
1F Azure Deployment Manager
2A Parameter file
2B Template function
2C Azure Reource Manager test toolkit
2D User-defined function
2E What-if operation
2F Azure Deployment Manager
Atype:dropdown
Answer:1C,2E
Box 1: Azure Resource Manager test toolkit

Use ARM template test toolkit -
The Azure Resource Manager template (ARM template) test toolkit checks whether your template uses recommended practices. When your template isn't compliant with recommended practices, it returns a list of warnings with the suggested changes. By using the test toolkit, you can learn how to avoid common problems in template development.

Box 2: What-if operation -
ARM template deployment what-if operation
Before deploying an Azure Resource Manager template (ARM template), you can preview the changes that will happen. Azure Resource Manager provides the what-if operation to let you see how resources will change if you deploy the template. The what-if operation doesn't make any changes to existing resources.
Instead, it predicts the changes if the specified template is deployed.
Reference:
https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/test-toolkit https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/deploy-what-if

https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/test-toolkit
https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/deploy-what-if?tabs=azure-powershell
https://4bes.nl/2020/08/09/testing-arm-templates/
***********************
N:60
T
You develop Azure Durable Functions to manage vehicle loans.
The loan process includes multiple actions that must be run in a specified order. One of the actions includes a customer credit check process, which may require multiple days to process.
You need to implement Azure Durable Functions for the loan process.
Which Azure Durable Functions type should you use?

1 orchestrator
2 client
3 entity
4 activity
ATYPE:SINGLESELECT
ANSWER:1
An orchestrator function is the appropriate type of Azure Durable Function to use in this scenario, because it allows you to define the overall flow of the loan process and call other functions or activities as needed. The credit check process can be implemented as a separate activity function, which can be called by the orchestrator function and run in parallel with other actions in the loan process.

Entity functions are designed for use cases where you need to perform operations on a shared piece of state in a reliable and atomic way, such as a distributed queue or counter. In this scenario, it does not appear that there is a need to use entity functions.

There is no such thing as a "client" function in Azure Durable Functions.

Activity functions are called by orchestrator functions to perform specific tasks, but the orchestrator function is the one that defines the overall flow of the loan process, so it is the correct answer in this case.

https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-sequence?tabs=csharp
***********************
N:61
T
You are developing an Azure Function app.
All functions in the app meet the following requirements:
(*) Run until either a successful run or until 10 run attempts occur.
(*) Ensure that there are at least 20 seconds between attempts for up to 15 minutes.

You need to configure the host.json file.

How should you complete the code segment?
{
  "dropdown1": {
    "strategy": "dropdown2",
    "dropdown3": 10,
    "minimumInterval": "00:00:20"
    "maximumInterval": "00:15:00"
  }
}


1A retry
1B healthMonitor
1C songleton
2A exponentialBackoff
2B counterThreshold
2C fixedDelay
3A maxRetryCount
3B healthCheckInterval
3C healthCheckThreshold
Atype:dropdown
Answer:1A,2A,3A
retry
exponentialBackoff
maxRetryCount
The "exponential backoff" retry strategy is a technique for retrying failed operations in a manner that avoids overloading the system being accessed. It works by increasing the amount of time that is waited between each retry attempt, using an exponential function to calculate the wait time.

https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-error-pages?tabs=exponential-backoff%2Ccsharp-script&pivots=programming-language-csharp#retry-strategies
***********************
N:61
T
You develop Azure Web Apps for a commercial diving company. Regulations require that all divers fill out a health questionnaire every 15 days after each diving job starts.

You need to configure the Azure Web Apps so that the instance count scales up when divers are filling out the questionnaire and scales down after they are complete.

You need to configure autoscaling.

What are two possible auto scaling configurations to achieve this goal?
1 Recurrence profile
2 CPU usage-based autoscaling
3 Fixed date profile
4 Predictive autoscaling
ATYPE:MULTISELECT
ANSWER:2,4
B and D
B. CPU usage-based autoscaling
This configuration uses the CPU usage of the web app to determine when to scale up or down. As divers fill out the health questionnaire, the web app will experience increased CPU usage, triggering autoscaling to increase the number of instances. Once the questionnaires are complete, CPU usage will decrease, triggering autoscaling to decrease the number of instances.

D. Predictive autoscaling:
This configuration uses historical data and machine learning algorithms to predict when to scale up or down. As the diving company collects more data on the health questionnaire process, the predictive autoscaling algorithm can use that data to predict when to scale up or down based on expected questionnaire submissions. This can help ensure that there are enough instances to handle the questionnaire submissions without overprovisioning and wasting resources.

A. Recurrence profile is used to schedule the scaling of resources at specific times or dates, but it does not meet the requirement to scale up when divers are filling out the questionnaire and scale down after they are complete. It only triggers scaling based on a set schedule, not based on actual usage.
C. Fixed date profile is used to specify the number of instances at a specific date and time, but it also does not meet the requirement to dynamically scale based on actual usage. It only sets a fixed number of instances and does not adjust based on changing workloads.

https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-predictive#predictive-autoscale-offerings
***********************
N:62
T
You are developing an online game that allows players to vote for their favorite photo that illustrates a word. The game is built by using Azure Functions and uses durable entities to track the vote count.
The voting window is 30 seconds. You must minimize latency.
You need to implement the Azure Function for voting.
How should you complete the code? 

[FunctionName("Vote")]
public static async Task<HttpResponseMessage> Run(
[HttpTrigger("POST", Route = "pic/{id}")] HttpRequestionMessage req,
[ ] string id)
{
var eid = new EntityId("pic", id);
await c.[ ] (eid, "vote");
return req.CreateResponse(HttpStatusCode.Ok);

1A CallEntityAsync
1B SignalEntityAsync
1C [DurableClient] IDurableEntityClient
1D [DurableClient] IDurableOrchestrationClient
2A CallEntityAsync
2B SignalEntityAsync
2C [DurableClient] IDurableEntityClient
2D [DurableClient] IDurableOrchestrationClient
Atype:dropdown
Answer:1C,2B
[DurableClient] IDurableEntityClient
SignalEntityAsync

https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-dotnet-entitiesbnjh
***********************
N:63
T
01 ClockBlockBlob src = null;
02 try
03 {
04   src = container.ListBlobs().OfType<CloudBlockBlob>().FirstOrDefault();
05   var id = await src.AquireLeaseAsync(null);
06   var dst = container.GetBlockBlobReference(src.Name);
07   string cpid = await dst.StartCopyAsync(src);
08   await dst.FetchAttributeAsync();
09  return id;
10 }
11 catch(Exception e)}
12 {
13   throw;
14 }
15 finally
16 {
17   if(src != null
18     await src.FetchAttributesAsync();
19   if(arc.Properties.LeaseState != Lease.Avaliable)
20     await src.Properties.LeaseAsync(new TimeSpan(0));
21 }

1 The code creates an infinite lease?
2 The code at line 06 always creates a new blob?
3 The finally block releases the lease?
ATYPE:MULTISELECT
ANSWER:1
A, not B, C

For a break operation, this is the proposed duration of seconds that the lease should continue before it is broken, between 0 and 60 seconds. This break period is only used if it is shorter than the time remaining on the lease. If longer, the time remaining on the lease is used. A new lease will not be available before the break period has expired, but the lease may be held for longer than the break period. If this header does not appear with a break operation, a fixed-duration lease breaks after the remaining lease period elapses, and an infinite lease breaks immediately.
 "infinite lease breaks immediately" is the key for this context.
https://docs.microsoft.com/en-us/rest/api/storageservices/lease-blob
https://www.c-sharpcorner.com/article/implementing-blob-leasing-understanding-blob-storage-part-9/

1) If acquires lease on the blob with null parameter, means the infinite lease is acquired [1]
2) On line 6 it gets reference to existing source blob, so no new blob is created there
3) It fails on start copy operation, because it tries to copy onto itself and the lease is already acquired
4) Goes to catch and then finally block
5) In finally breaks the lease with (Zero) parameter that means the immediate lease break [2]

https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblobcontainer.acquireleaseasync?view=azure-dotnet-legacy#microsoft-azure-storage-blob-cloudblobcontainer-acquireleaseasync(system-nullable((system-timespan))-system-string)
https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblobcontainer.breakleaseasync?view=azure-dotnet-legacy#microsoft-azure-storage-blob-cloudblobcontainer-breakleaseasync(system-nullable((system-timespan)))
***********************
N:64
T
You are building a website that uses Azure Blob storage for data storage. You configure Azure Blob storage lifecycle to move all blobs to the archive tier after 30 days.
Customers have requested a service-level agreement (SLA) for viewing data older than 30 days.
You need to document the minimum SLA for data recovery.
Which SLA should you use?
1 at least two days
2 between one and 15 hours
3 at least one day
4 between zero and 60 minutes
ATYPE:SINGLESELECT
ANSWER:2
Correct Answer: B

- Standard priority: The rehydration request will be processed in the order it was received and may take up to 15 hours.
- High priority: The rehydration request will be prioritized over Standard requests and may finish in under 1 hour for objects under ten GB in size.

Reference:

https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers?tabs=azure-portal#archive-access-tier

https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-rehydration?tabs=azure-portal

https://docs.microsoft.com/en-us/azure/storage/blobs/archive-rehydrate-overview#rehydration-priority

https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-rehydration?tabs=azure-portal#rehydrate-an-archived-blob-to-an-online-tier
***********************
N:65
T
You are developing a ticket reservation system for an airline.
The storage solution for the application must meet the following requirements:
(*) Ensure at least 99.99% availability and provide low latency.
(*) Accept reservations even when localized network outages or other unforeseen failures occur.
(*) Process reservations in the exact sequence as reservations are submitted to minimize overbooking or selling the same seat to multiple travelers.
(*) Allow simultaneous and out-of-order reservations with a maximum five-second tolerance window.
You provision a resource group named airlineResourceGroup in the Azure South-Central US region.
You need to provision a SQL API Cosmos DB account to support the app.
How should you complete the Azure CLI commands?

resourceGroupName='airlineResourceGroup'
name='docdb-airline-reservations'
databaseName='docdb-tickets-database'
colletionName='docdb-tickets-collestion'
consitencyLevel=dropdown1
az cosmosdb create --name $name [dropdown2]
--resource-group $resourceGroupName --max-interval 5 dropdown3
--default-consistency-level = $consistencyLevel

1A Strong
1B Eventual
1C ConsistantPrefix
1D BoundedStaleness
2A --enable-virtual-network true
2B --enable-automatic-failover true
2C --kind 'GlobalDocumentDB'
2D --kind 'MongoDB'
3A --locations 'southcentralus'
3B --locations 'eastus'
3C --locations 'southcentralus=0 eastus=1 westus=2'
3D --locations 'southcentralus=0'
Atype:dropdown
Answer:1D,2B,3C
BoundedStaleness
--enable-automatic-failover true
--locations 'southcentralus=0 eastus=1 westus=2'

max-interval, indicates this must be bounded-slateness, enable-automatic-failover, indicated this must be multi-region

there is a --max-interval property which is being used with bounded staleness only
there is a requirement "Accept reservations event when localized network outages or other unforeseen failures occur." which points us to multiple writes for multiple regions which is not being supported by the Strong consistency.
--locations syntax looks like obsolete as for late march 2021

Box 1: BoundedStaleness -
Bounded staleness: The reads are guaranteed to honor the consistent-prefix guarantee. The reads might lag behind writes by at most "K" versions (that is,
"updates") of an item or by "T" time interval. In other words, when you choose bounded staleness, the "staleness" can be configured in two ways:
The number of versions (K) of the item
The time interval (T) by which the reads might lag behind the writes
Incorrect Answers:

Strong -
Strong consistency offers a linearizability guarantee. Linearizability refers to serving requests concurrently. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.
Box 2: --enable-automatic-failover true\
For multi-region Cosmos accounts that are configured with a single-write region, enable automatic-failover by using Azure CLI or Azure portal. After you enable automatic failover, whenever there is a regional disaster, Cosmos DB will automatically failover your account.
Question: Accept reservations event when localized network outages or other unforeseen failures occur.
Box 3: --locations'southcentralus=0 eastus=1 westus=2
Need multi-region.

https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels
https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/cosmos-db/manage-with-cli.md

https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels#strong-consistency-and-multiple-write-regions

https://docs.microsoft.com/en-us/cli/azure/cosmosdb?view=azure-cli-latest#az_cosmosdb_create
***********************
N:66
T
You are preparing to deploy a Python website to an Azure Web App using a container. The solution will use multiple containers in the same container group.
The Dockerfile that builds the container is as follows:

FROM python:3
ADD website.py
CMD [ "python", "./website.py" ]

You build a container by using the following command. The Azure Container Registry instance named images is a private registry.

docker build -t images.azurecr.io/website:v1.0.0

The user name and password for the registry is admin.
The Web App must always run the same version of the website regardless of future builds.
You need to create an Azure Web App to run the website.
How should you complete the commands?

az configure --default web=website
az configure --defaults group=website
az appservice plan create --name websitePlan dropdown1
az webapp create --plan websitePlan dropdown2
az webapp config dropdown3


1A --sku SHARED
1B --tags container
1C --sku B1 --hyper-v
1D --sku B1 --is-linux
2A --deployment-source-url images.azurecr.io/website:v1.0.0
2B --deployment-source-url images.azurecr.io/website:latest
2C --deployment-container-image-name images.azurecr.io/website:v1.0.0
2D --deployment-container-image-name images.azurecr.io/website:latest
3A set --python-version 2.7 --generic-configurations user=admin password=admin
3B set --python-version 3.6 --generic-configurations user=admin password=admin
3C container set --docker-registry-server-url https://images.azurecr.io -u admin -p admin
3D container set --docker-registry-server-url https://images.azurecr.io/website -u admin -p admin
Atype:dropdown
Answer:1D,2C,3C
--sku B1 --is-linux
--deployment-container-image-name images.azurecr.io/website:v1.0.0
-- container set --docker-registry-server-url https://images.azurecr.io -u admin -p admin

"use multiple containers in the same container group" this not is possible in windows.
Solution is:
--is-linux
--deployment-container-image-name
"Multi-container groups are currently restricted to Linux containers."
https://docs.microsoft.com/en-us/azure/container-instances/container-instances-multi-container-group

https://www.linkedin.com/pulse/how-quickly-create-micro-service-azure-webapp-fastapi-bonnet-?trk=pulse-article_more-articles_related-content-card

https://docs.microsoft.com/en-us/cli/azure/appservice/plan

https://docs.microsoft.com/en-us/azure/app-service/tutorial-custom-container?pivots=container-linux

https://docs.microsoft.com/en-us/azure/container-instances/container-instances-container-groups
***********************
N:67
T
You are developing a back-end Azure App Service that scales based on the number of messages contained in a Service Bus queue.
A rule already exists to scale up the App Service when the average queue length of unprocessed and valid queue messages is greater than 1000.
You need to add a new rule that will continuously scale down the App Service as long as the scale up condition is not met.
How should you configure the Scale rule?

Scale Rule
Metric source
dropdown1
Resource type
Service Bus Namespaces
Resource
MessageQueue1103
Queues
itemqueue

Critera
Metric name
dropdown2

Time grain statistic
dropdown3

1A Storage queue
1B Service Bus queue
1C Current resource
1D Storage queue (classic)
2A Message count
2B Active Message Count
3A 3A Total
3B Maximum
3C Average
3D Count
Atype:dropdown
Answer:1B,2B,3C
1) Service bus queue
2) Active message count
3) Average
4) Less than or equal to
5) Decrease count by

https://vceguide.com/wp-content/uploads/2019/10/Microsoft-AZ-203-date-01-06-2019-00001_Page_062_Image_0001.jpg

Box 1: Service bus queue
You are developing a back-end Azure App Service that scales based on the number of messages contained in a Service Bus queue.

Box 2: ActiveMessage Count
ActiveMessageCount: Number of messages in the queue or subscription that are in the active state and ready for delivery.

Box 3: Average
For special metrics such as Storage or Service Bus Queue length metric, the threshold is the average number of messages available per current number of instances.

Box 4: Less than or equal to
You need to add a new rule that will continuously scale down the App Service, as long as the scale up condition is not met.

Box 5: Decrease count by

https://docs.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-best-practices#considerations-for-scaling-threshold-values-for-special-metrics

https://docs.microsoft.com/en-us/azure/service-bus-messaging/message-counters
***********************
N:68
T
You have an application that uses Azure Blob storage.
You need to update the metadata of the blobs.
Which three methods should you use to develop the solution?

1 Metadata.Add
2 SetMetadataAsync
3 FetchAttributesAsync
4 UploadFileStream
5 SetPropertiesAsync
ATYPE:MULTISELECT
ANSWER:1,2,3
1,2,3
Since we're talking about updating the metadata,
- first we need to fetch it, to populate blob's properties and metadata (we want to update it - without fetching we would just set the new metadata):
FetchAttributesAsync
- second, we need to manipulate the metadatas to update them and the best fitting is
Metadata.Add
- third, we have to persist our changes. We can use a method that initiates an asynchronous operation to update the blob's metadata, which is
SetMetadataAsync

FetchAttributesAsync is version 11 (legacy)
In version 12 is would be :
1. await blob.GetPropertiesAsync();
2. metadata.Add("docType", "textDocuments");
3. await blob.SetMetadataAsync(metadata);
https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-properties-metadata
***********************
N:69
T
You are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce
2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.
You must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.
You need to implement a solution to receive the device data.
1 Provision an Azure Event Grid. Configure the machine identifier as the partition key and enable capture.
2 The solution is not preferred
ATYPE:MULTISELECT
ANSWER:2
Not solution 1, Its Event Hub not, Grid. Azure Event Hub can be used to get the messages from the various devices. Azure Event Hub capture can then be used to persist the events to Azure Blob storage.
"enable capture". I have found capture only for Event Hub, not for Event Grid.

Azure Event Grid is an event routing service that allows you to handle events from various Azure services and your own applications. It can be used to send events from an application to multiple subscribers, but it is not well suited for receiving data from thousands of devices and storing them in Azure Blob storage.
Azure Event Grid is also not meant for long-term data storage and it is not a good fit for this scenario, where large amounts of data need to be stored and correlated based on a device identifier.

https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview
***********************
N:69
T
You develop Azure solutions.
A .NET application needs to receive a message each time an Azure virtual machine finishes processing data. The messages must NOT persist after being processed by the receiving application.
You need to implement the .NET object that will receive the messages.
Which object should you use?

1 QueueClient
2 SubscriptionClient
3 TopicClient
4 CloudQueueClient
ATYPE:MULTISELECT
ANSWER:1
Correct Answer: A
So, the question is really about what kind of queue message tool you should use. And the key word here is that "message must NOT persist after being processed".

Microsoft.AzureService.Bus.QueueClient as it supports "At-Most-Once" deliver mode while Azure.Storage.Queues.CloudQueueClient doesn't.

https://docs.microsoft.com/en-us/dotnet/api/azure.storage.queues.queueclient?view=azure-dotnet

https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.queue.cloudqueueclient?view=azure-dotnet-legacy
***********************
N:70
T
You are maintaining an existing application that uses an Azure Blob GPv1 Premium storage account. Data older than three months is rarely used.
Data newer than three months must be available immediately. Data older than a year must be saved but does not need to be available immediately.
You need to configure the account to support a lifecycle management rule that moves blob data to archive storage for data not modified in the last year.
Which three actions should you perform in sequence?
group-1-start
Upgrade the storage account to GPv2
group-1-end
group-2-start
Create a new GPv2 Standard account and set default access tier level to cool
group-2-end
group-3-start
Change the storage account access tier from hot to cool
group-3-end
group-4-start
Copy the data to be arcived to a Standard GPv2 storage account and then delete the data from the original storage account
group-4-end
ATYPE:ORDER
ANSWER:1,2,4
Since we already have a premium P1 account with gpv1. Why not:
- Upgrade the existing one to GPv2
- Create a new GPV2 standard account with default access level to cool
- And then copy archive data to the GPV2 and delete the data from original storage account.

General Purpose v1 (GPv1) accounts don't support tiering.

https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers

https://docs.microsoft.com/en-us/azure/storage/common/storage-account-upgrade?tabs=azure-portal

You can't create the second account in a cool tier because of this:
Data stored in a premium block blob storage account cannot be tiered to hot, cool, or archive using Set Blob Tier or using Azure Blob Storage lifecycle management. To move data, you must synchronously copy blobs from the block blob storage account to the hot tier in a different account using ...

https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview#blob-lifecycle-management
***********************
N:71
T
You develop Azure solutions.
You must connect to a No-SQL globally-distributed database by using the .NET API.
You need to create an object to configure and execute requests in the database.
Which code segment should you use?
1 new Container(EndpointUri, PrimaryKey);
2 new Database(EndpointUri, PrimaryKey);
3 new CosmosClient(EndpointUri, PrimaryKey);
ATYPE:SINGLESELECT
ANSWER:2
Correct Answer: C

Azure Cosmos DB is a fully managed NoSQL database for modern app development. Single-digit millisecond response times, and automatic and instant scalability, guarantee speed at any scale.

// Create a new instance of the Cosmos Client
this.cosmosClient = new CosmosClient(EndpointUri, PrimaryKey)
//ADD THIS PART TO YOUR CODE
await this.CreateDatabaseAsync();

https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.cosmosclient?view=azure-dotnet

https://docs.microsoft.com/en-us/azure/cosmos-db/sql-api-get-started

There are two ways by which we can created cosmosclient instance.
1. CosmosClient cosmosClient = new CosmosClient(connectionString);
2. CosmosClient cosmosClient = new CosmosClient(EndpointUri, PrimaryKey)
***********************
N:72
T
You have an existing Azure storage account that stores large volumes of data across multiple containers.
You need to copy all data from the existing storage account to a new storage account. The copy process must meet the following requirements:
(*) Automate data movement.
(*) Minimize user input required to perform the operation.
(*) Ensure that the data movement process is recoverable.
What should you use?
1 AzCopy
2 Azure Storage Explorer
3 Azure portal
4 .NET Storage Client Library
ATYPE:MULTISELECT
ANSWER:2,3
Correct Answer: C

Azure Storage Explorer uses AzCopy to perform all of its data transfer operations. But in this questions, there is a requirement to minimize user interaction which is why AzCopy is more appropriate.

Reference:

https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10

https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-copy
***********************
N:72
T
You are developing a web service that will run on Azure virtual machines that use Azure Storage. You configure all virtual machines to use managed identities.
You have the following requirements:
(*) Secret-based authentication mechanisms are not permitted for accessing an Azure Storage account.
(*) Must use only Azure Instance Metadata Service endpoints.
You need to write code to retrieve an access token to access Azure Storage. 

var url = "dropdown1"

var queryString = "";
var client = new HttpClient();
var response = await client.GetAsync(url + queryString);
var payload = await response.Content.ReadAsStringAsync();
return dropdown2

1A http://localhost:50342/oauth2/token
1B http://169.254.169.254:50342/oauth2/token
1C http://localhost/metadata/identity/oauth2/token
1D http://169.254.169.254/metadata/identity/oauth2/token
2A XDocument.Parse(payload)
2B new MultipartContent(payload)
2C new NetworkCredetial("Azure", payload)
2D JsonConvert.DeserializeObject<Dictionary<string, string>>(payload)
Atype:dropdown
Answer:1D,2D
Box 1: http://169.254.169.254/metadata/identity/oauth2/token
Sample request using the Azure Instance Metadata Service (IMDS) endpoint (recommended):
GET 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/' HTTP/1.1 Metadata: true

Box 2: JsonConvert.DeserializeObject<Dictionary<string,string>>(payload);
Deserialized token response; returning access code.

https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token

https://docs.microsoft.com/en-us/azure/service-fabric/how-to-managed-identity-service-fabric-app-code
***********************
N:73
T
You are developing a new page for a website that uses Azure Cosmos DB for data storage. The feature uses documents that have the following format:
{
"name":"john",
"city": seattle
}
You must display data for the new page in a specific order. You create the following query for the page:

SELECT * from people p order by p.name, p.city desc

You need to configure a Cosmos DB policy to support the query.
How should you configure the policy?

{
"automatic":true,
"ngMode": "Consistent",
"includePaths":[{"path": "/*}]
"excludePaths":[],
"dropdown1" : [
[
  {"path": "name", order: descending"},
  {"path": "city", order: dropdown2"},
]}

1A orderBy
1B sortOrder
1C ascending
1D descending
1E compositeIndexes
2A orderBy
2B sortOrder
2C ascending
2D descending
2E compositeIndexes
Atype:dropdown
Answer:1E,2C
Answer is compositeIndexes AND ascending

Box 2 is "ascending"
See explanation here:
https://docs.microsoft.com/en-us/azure/cosmos-db/index-policy#order-by-queries-on-multiple-properties
***********************
N:73
T
You are building a traffic monitoring system that monitors traffic along six highways. The system produces time series analysis-based reports for each highway.
Data from traffic sensors are stored in Azure Event Hub.
Traffic data is consumed by four departments. Each department has an Azure Web App that displays the time series-based reports and contains a WebJob that processes the incoming data from Event Hub. All Web Apps run on App Service Plans with three instances.
Data throughput must be maximized. Latency must be minimized.
You need to implement the Azure Event Hub.
Which settings should you use?

Number of partitions dropdown1
Partition key dropdown2

1A 3
1B 4
1C 6
1D 12
2A Highway
2B Department
2C Timespamp
2D VM name
Atype:dropdown
Answer:1C,2A
6
Highway

Partitions relate to producers - and the logical way to partition the incoming data is by the only value you have at that point, the highway name/id. So the selected answer is correct (6 Partitions, by Highway).

People are getting confused by the departments which would actually each be an event consumer with an associated Consumer Group which would have it's own isolated view of each of the highway partitions.

https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-faq

https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/event-hubs/partitioning-in-event-hubs-and-kafka#distribute-events-to-partitions

https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-features#partitions

https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features#publishing-an-event
***********************
N:74
T
You are implementing an order processing system. A point of sale application publishes orders to topics in an Azure Service Bus queue. The Label property for the topic includes the following data:

Property   Description
ShipLocation   The country where the order will be shipped
CorrelationId  A priority value for the order
Quantity       A user-defined field that stors the quantity of items in an order.
AuditedAt      A user-defined field that records the date an order is audited

Subscripition type    Comments
FutureOrders          This subscription is reserved for future use and must not receive any orders 
HighPriorityOrders    Handle all high priority orders and international orders
InternatialOrders     Handle orders where the country/region is not United States
HighQuantityOrders    Handle only orders with quantities greater that 100 units
AllOrders             This subscription is used for auditing purposes. This subscription must receive every single order. AllOrders has an Action defined that updates the AudiedAl property to include the date and time it was received by the subscription.

You need to implement filtering and maximize throughput while evaluating filters.
Which filter types should you implement? 

FutureOrders [SQLFilter / CorrelationFilter / No Filter]
HighPriorityOrders [SQLFilter / CorrelationFilter / No Filter]
InternatialOrders [SQLFilter / CorrelationFilter / No Filter]
HighQuantityOrders [SQLFilter / CorrelationFilter / No Filter]
AllOrders [SQLFilter / CorrelationFilter / No Filter]

1A SQLFilter
1B CorrelationFilter
1C No Filter
2A SQLFilter
2B CorrelationFilter
2C No Filter
3A SQLFilter
3B CorrelationFilter
3C No Filter
4A SQLFilter
4B CorrelationFilter
4C No Filter
5A SQLFilter
5B CorrelationFilter
5C No Filter
Atype:dropdown
Answer:1A,2B,3A,4A,5C
-SQL filter (just false) Future Orders, doesn't return anything
-Correlation Filter (Priority is CorrelateID) check for equality
-SQL filter (country != us) not US, hence sql Filter
-SQL filter (size>100) to use with 'greater than'
-No Filter to return everything.

SQL (just false)
SQL (priority>min priority)
SQL (country != us)
SQL (size>100)
No filter

https://learn.microsoft.com/en-us/azure/service-bus-messaging/topic-filters

https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-filter-examples
***********************
N:74
T
Your company has several websites that use a company logo image. You use Azure Content Delivery Network (CDN) to store the static image.
You need to determine the correct process of how the CDN and the Point of Presence (POP) server will distribute the image and list the items in the correct order.
In which order do the actions occur?
group-1-start
If no edge servers in the POP have the immage in cache, the POP requests the file from the origin server.
group-1-end
group-2-start
A user requests the image from the CDN URL. The NDS routes the request to the best performing POV location.
group-2-end
group-3-start
Subsequent requests for the file may be directed to the same POP using the CDN logo image URL. The POP edge server returns the file from cache if the TTL has not expired.
group-3-end
group-4-start
The origin server returns the file to an edge server in the POP. An edge server in the POP caches the logo image and returns the image to the client.
group-4-end
ATYPE:ORDER
ANSWER:1,2,3,4
blablabla blablabla
***********************
N:75
T
You are developing an Azure Cosmos DB solution by using the Azure Cosmos DB SQL API. The data includes millions of documents. Each document may contain hundreds of properties.
The properties of the documents do not contain distinct values for partitioning. Azure Cosmos DB must scale individual containers in the database to meet the performance needs of the application by spreading the workload evenly across all partitions over time.
You need to select a partition key.
Which two partition keys can you use? Each correct answer presents a complete solution.
1 a single property value that does not appear frequently in the documents
2 a value containing the collection name
3 a single property value that appears frequently in the documents
4 a concatenation of multiple property values with a random suffix appended
5 a hash suffix appended to a property value
ATYPE:MULTISELECT
ANSWER:4,5
D and E
You can form a partition key by concatenating multiple property values into a single artificial partitionKey property. These keys are referred to as synthetic keys.
Another possible strategy to distribute the workload more evenly is to append a random number at the end of the partition key value. When you distribute items in this way, you can perform parallel write operations across partitions.
Note: It's the best practice to have a partition key with many distinct values, such as hundreds or thousands. The goal is to distribute your data and workload evenly across the items associated with these partition key values. If such a property doesn't exist in your data, you can construct a synthetic partition key.

If no property in the document data will have unique values, you need to make one.
This is called a synthetic partition key.
These sorts of keys are made by adding a unique suffix at the end of some property.
One other way is to create a property that will have the hashed data + a random suffix.
The objective is to have a property that is random enough so that you can rely on it to be your key.

https://docs.microsoft.com/en-us/azure/cosmos-db/synthetic-partition-keys
***********************
N:76
T
You are developing an Azure-hosted e-commerce web application. The application will use Azure Cosmos DB to store sales orders. You are using the latest SDK to manage the sales orders in the database.
You create a new Azure Cosmos DB instance. You include a valid endpoint and valid authorization key to an appSettings.json file in the code project.
You are evaluating the following application code: (Line number are included for reference only.)

IMAGE: images\question_3_19.png

A database with dropdown1 'SalesOrder' is created. The databse will include dropdown2 containers.
Container1 will contain dropdown3 items
Container2 will contain dropdown4 items
1A id
1B name
2A One
2B Two
2C Three
3A One
3B Two
3C Three
4A One
4B Two
4C Three
Atype:dropdown
Answer:1A,2B,3B,4A
Id=SalesOrder
Two containers
Container1 will contain two items
Container2 will contain One item

https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.cosmosclient.createdatabaseifnotexistsasync

https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.database.createcontainerasync

https://docs.microsoft.com/en-us/dotnet/api/azure.cosmos.cosmoscontainer.createitemasync
***********************
N:77
T
You develop an Azure solution that uses Cosmos DB.
The current Cosmos DB container must be replicated and must use a partition key that is optimized for queries.
You need to implement a change feed processor solution.
Which change feed processor components should you use?

Requirement => Component
Store the data from which the change feed is generated => dropdown1
Coordinate processing of the change feed across multiple workers => dropdown2
Use the change feed processors to listen for changes => dropdown3
Handle each batch of changes => dropdown4
1A Host
1B Delegate
1C Lease container
1D Monitored container
2A Host
2B Delegate
2C Lease container
2D Monitored container
3A Host
3B Delegate
3C Lease container
3D Monitored container
4A Host
4B Delegate
4C Lease container
4D Monitored container
Atype:dropdown
Answer:1D,2C,3A,4B
Box 1: The monitored container -
The monitored container has the data from which the change feed is generated. Any inserts and updates to the monitored container are reflected in the change feed of the container.

Box 2: The lease container -
The lease container acts as a state storage and coordinates processing the change feed across multiple workers. The lease container can be stored in the same account as the monitored container or in a separate account.

Box 3: The host: A host is an application instance that uses the change feed processor to listen for changes. Multiple instances with the same lease configuration can run in parallel, but each instance should have a different instance name.

Box 4: The delegate -
The delegate is the code that defines what you, the developer, want to do with each batch of changes that the change feed processor reads.
Reference:
https://docs.microsoft.com/en-us/azure/cosmos-db/change-feed-processor
***********************
N:78
T
You are developing a web application that will use Azure Storage. Older data will be less frequently used than more recent data.
You need to configure data storage for the application. You have the following requirements:
(*) Retain copies of data for five years.
(*) Minimize costs associated with storing data that is over one year old.
(*) Implement Zone Redundant Storage for application data.
What should you do?

Requirement => Solution
Configure an azure storage account => dropdown1
Configure data retention => dropdown2
1A Implement Blob Storage
1B Implement Azure Cosmos DB
1C Implement Storage (general purpose v1)
1D Implement StorageV2 (general purpose v2)
2A Snapshot blobs and move them to the archive tier
2B Set a lifecycle management policy to move blobs to the cool tier
2C Use AzCopy to copy the data to an on-premises device for backup
2D Set a lifecycle management policy to move blobs to the archive tier
Atype:dropdown
Answer:1D,2B
Implement StorageV2 (general purpose v2)
Set a lifecycle management policy to move blobs to the cool tier

Only storage accounts that are configured for LRS, GRS, or RA-GRS support moving blobs to the Archive tier. The Archive tier isn't supported for ZRS, GZRS, or RA-GZRS accounts.
https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview?source=recommendations

https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy

https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview

https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers

https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy?toc=/azure/storage/blobs/toc.json
***********************
N:79
T
A company develops a series of mobile games. All games use a single leaderboard service.
You have the following requirements:
(*) Code must be scalable and allow for growth.
(*) Each record must consist of a playerId, gameId, score, and time played.
(*) When users reach a new high score, the system will save the new score using the SaveScore function below.
Each game is assigned an Id based on the series title.

You plan to store customer information in Azure Cosmos DB. The following data already exists in the database:
IMAGE: images\question_3_22_0.png

You develop the following code to save scores in the data store. (Line numbers are included for reference only.)
IMAGE: images\question_3_22_1.png
You develop the following code to query the database. (Line numbers are included for reference only.)
IMAGE: images\question_3_22_2.png

1 SaveScore will work with Cosmos DB.
2 SaveScore will update and replace a record if one already exists with the same player and gameId.
3 Leader board data for the game will be automatically partitioned using gameId.
4 SaveScore will store the values for the gameId and playerId parameters in the database.
ATYPE:MULTISELECT
ANSWER:1,4
1,4
Code for CosmosDB Yes
Save score func No
Data partitioned No
Code store parametres Yes

If partitionKey is mentioned in the Code, then Data partitioned is YES

https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.table.cloudtableclient?view=azure-dotnet
***********************
N:80
T












































